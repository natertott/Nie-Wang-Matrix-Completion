\subsubsection{Design of Experiments}
Parametric analysis, broadly defined, is an experimental method of mapping independent variables to their corresponding dependent parameters. Process-property relationships are typically studied through parametric analysis. Machine learning aids in investigations of AM by reducing the amount of experiments needed to characterize process-property relationships. Machine learning approaches like sequential learning model relationships in parametric studies to discover regions of the parameter space which will produce the most information about process-property relationships. 

 In additive manufacturing independent parameters such as laser energy, speed, build direction, composition, layer height, and more are varied to study their impact on material properties. Examples include relating build geometry to microstructure or surface roughness \cite{Antonysamy2013, Strano2013} or temperature history to microstructure \cite{Bontha2009, Nie2014}, or substrate temperature to residual stress development \cite{Chen2016, Brice2018}, or even entire manufacturing processes to microstructure \cite{Baufeld2011}. One of the most common types of parametric analysis is relating heat source parameters to all aspects of AM, such as part temperature history \cite{Bontha2006, Li2014}, microstructure \cite{Cherry2015, Jia2014}, mechanical properties \cite{Delgado2012, Khorasani2018}, residual stresses \cite{Wu2014, Denlinger2015}, and more.

Both engineering and scientific investigations of AM utilize parametric analysis. In the sciences, parametric analysis proceeds until a theory or model can be presented for a process-property relationship. In engineering, parametric analysis continues until an optimality criterion is met, such as maximum strength or minimum porosity. Both disciplines vary independent parameters and measure dependent responses; doing so provides information about the underlying phenomenon. 

\textit{Information} is any observation of process-property relationships. For example, observing that a set of laser parameters results in an equiaxed microstructure can be considered information because the researcher has gained an idea of the properties to expect from set processing conditions. Therefore, \textit{information gain} is any experiment which reveals a previously unobserved process-property relationship. Rigorous mathematical definitions of information and information gain have been defined, typically referencing back to Shannon's original formulation of information theory \cite{Shannon1948}.

Traditional design of experiments maximize information gain by dividing up parameter spaces to maximize distance between like experimental conditions. Machine learning driven design of experiments makes suggestions for future experiments based on the results of past experiments. It does not make any assumptions up front about correlations between design parameters. Rather, machine learning algorithms model relationships between inputs and outputs and suggest the experiment which is statistically most likely to result in information gain. Design of experiments with machine learning algorithms can be adopted by augmenting traditional design of experiments with a statistical model.

The first step is to define the space of parameters which may impact properties, as is done in traditional design of experiments. The design space is all possible combinations of process parameters. As more parameters and finer step sizes are added, the size of the design space grows. Once the design space has been defined, the next step is to generate an initial dataset. The process-property relationships revealed in these initial tests will be the basis of an initial statistical model. 

After an initial dataset is generated, the researchers need to define a \textit{response function} which interprets the relationship between parameters and material properties. One example is a regression model of the process parameters and material characteristics. Identifying tests of importance begins by identifying all $n$ parameters in the design space that may or may not impact a final manufacturing result $y$. First, a subset of the $n$ many parameters $m_1$ is chosen such that at least one parameter in $\mathbf{x}$ is left out. A simple regression function called a decision tree is trained
\begin{equation}
	\hat{f}_1(\mathbf{x}_{m_1}) = y
\end{equation}
on the subset of features $m_1$. Next, a new random sampling of the $n$ parameters $m_2$ is chosen, and a second regression problem is solved $\hat{f}_2(\mathbf{x}_{m_2}) = y$. The random forest is a linear combination of all of these individual regression functions
\begin{equation}
	\hat{f}(\mathbf{x}) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}_b(\mathbf{x}_{m_b})
	\label{randomforest}
\end{equation}
out of $B$ many parameter samplings. A standard deviation in the prediction output is typically computed as
\begin{equation}
	\sigma = \sqrt{\frac{\sum_{b=1}^B \hat{f}_b(\mathbf{x}_{m_b}) - \hat{f}(\mathbf{x})}{B-1}}
	\label{rfstddev}
\end{equation}
Some of the features in $\mathbf{x}$ may be irrelevant for studying process-property relationships and some may be very important to model. Random forests test the importance of an AM process parameter in $\mathbf{x}$ by re-training every individual regression function $\hat{f}_b(\mathbf{x}_{m_b})$ with that machine input included. Then, the standard deviation is re-computed. If the inclusion of a parameter in every regression tree significantly changes $\sigma$ then it plays a large role in prediction of $y$. Therefore, the feature should be included and tested in simulations.

Ling et al. performed design of experiments to optimize the process of designing high fatigue strength steels. \cite{Ling2017a}. In Ling's case, a random forest model was fit to design criteria from a database of steel compositions, processing routes, and fatigue strength. The goal was to find the composition and processing route combination which had the highest fatigue strength in the dataset using as few experiments as possible. A benefit of random forest models is that they provide uncertainty estimates on predictions. This means that regions of the design space with high uncertainty can be identified.

Random forests are easy to use because they do not require tuning many hyper-parameters. Furthermore, random forests are very good at sifting through many irrelevant features to find the features that actually matter. Random forest training is also computationally inexpensive and easily parallelized. Random forests provide two advantages that are particularly important in the context of materials science: the ability to efficiently calculate uncertainty estimates and the added interpretability of feature importance metrics. Based on their ensemble nature, it is possible to generate uncertainty estimates for random forest predictions using jackknife-based procedures~\cite{Efron1992, Efron2014, Wager2014}. They automatically identify the most important features in a given model based on how often a feature is used in splitting criteria and what the aggregated information gain is over those splits. These feature importance metrics can provide insights into how the model is making its predictions. Random forests have been applied successfully to a range of applications in materials science. They have been used to discover new Heusler compounds~\cite{Oliynyk2016} and new thermoelectric materials~\cite{Gaultois2016}. They have also been used to model material properties such as thermal conductivity in half-Heusler semiconductors ~\cite{Carrete2014} and to break down fields for dielectrics~\cite{Kim2016}. 

Ling's machine learning-assisted design of experiments proceeded by suggesting experiments with a high uncertainty in their result as modeled by the random forest regression. The intuition is that predictions by the response function which have low uncertainty have enough data to characterize the process-property relationships in that region of the design space. Therefore, researchers can have high confidence in the material properties they will achieve if parts are printed at those conditions. Thus, the process-property relationship likely has enough information to pick an optimal condition or investigate further. Regions of the design space with high uncertainty do not have enough information therefore further experiments are required. Thus, machine learning algorithms suggest these regions of the design space for further experiments. When only varying a few parameters at a time, regions of the design space which need characterization can be easily identified. When varying tens of parameters in additive manufacturing these regions of the design space are not apparent. Furthermore, correlated inputs can be masked by the complexity of the process-property relationship. Statistical models can spot these correlated regions of the design space.

A regression model, such as that used by Ling, is only one way of assessing process-property relationships. A review article detailing many optimization algorithms for design of experiments can be found in Shan et al. \cite{Shan2010}. Adoption of machine-learning assisted design of experiments algorithms can rapidly increase the rate at which the relationship between AM process parameters and material properties are understood. 
