

\subsubsection{Surrogate Modeling}
Phenomenon which are difficult to study experimentally, such as flow within the melt pool, are best studied through modeling approaches. Though expensive, full-physics modeling is quite necessary to understand how physics at different scales interact to impact the AM process. Full physics models, however, can be expensive and time consuming to run.  If a model's computational expense is \textit{too} high then performing simulations at all relevant manufacturing conditions can be impossible. Dimensionally reduction may not be an option. While it is useful for optimization and visualization, reduced order models are unlikely to capture the full dynamics of solidification in AM. Machine learning algorithms can use the results of previously run high fidelity simulations to fill in the gaps and reduce development time.

A \textit{surrogate model} is a regression model that interpolates the results of high-cost simulations. Surrogate models are regressed on the inputs and results of previously run simulations. Then, the surrogate model interpolates simulation results at new coordinates the design space. Surrogate models preclude the need for running computationally expensive simulations for every possible manufacturing condition. Surrogates can be as simple as linear regression between simulation inputs and results, but are often more complex. The accuracy of a surrogate model is dependent upon how many previous simulations have been run and at how many different points in the design space.

Tapia et al. built a surrogate model for laser powder bed fusion of 316L stainless steel. They were concerned with predicting the melt pool depth of single-track prints solely from the laser power, velocity, and spot size \cite{Tapia2017}. The dataset used to build the surrogate was computationally derived, based on previous simulation methods used by the same research team \cite{King2014}. In particular, they used the results from a computationally expensive but high-accuracy melt pool flow model of Khairallah et al. \cite{Khairallah2016}. They ran powder bed simulations at various laser powers, velocities, and spot sizes, and the model told them the depth of the melt pool, among other information. The datasets provided enough information for a surrogate model to be trained to predict simulation results.

To build surrogate model, Tapia et al. used a machine learning model known as a Gaussian process model (GPM). A common model assumption in Gaussian process modeling is
\begin{equation}
	z(\mathbf{x}) = y(\mathbf{x}) + \epsilon(\mathbf{x})
	\label{model}
\end{equation}
where $y(\mathbf{x})$ is the approximation (surrogate) of the simulation process, $\epsilon(\mathbf{x})$ is a stochastic, randomly distributed noise in measurement, and $z(\mathbf{x})$ is the value given by a simulation. GPMs make the assumption that all finite joint distributions of $y(\mathbf{x})$ are distributed in a multivariate normal manner, also known as a Gaussian distribution. For AM modeling this assumption holds if the model data used for a surrogate is stochastic in nature. The primary goal in GPMs is to find model parameters for the mean process $y(\mathbf{x})$ and a covariance function $\kappa(\mathbf{x},\mathbf{x}')$, which is a function of similar form to Eqn. \ref{gausskernel}. Fitting a Gaussian process model often begins with assuming a model function for covariance, fitting the model parameters such as $\sigma$ to the observed values $z(\mathbf{x})$, then using those model parameters to predict simulation results $y(\mathbf{x})$ at other locations in the design space. 

Tapia used Bayesian statistics to develop a probabilistic model that predicted melt pool depth from simulation inputs. They were able to successfully predict the outcomes of both high-fidelity simulations and experimental measurements solely by analyzing trends in previously obtained results. In particular, they were able to accurately predict the melt pool depth at a value that had never been observed before, either computationally or experimentally. For future investigations, predictions by the surrogate model can be relied up on instead of running a simulation or experiment. 

Gaussian process models have benefits beyond their surrogate modeling capabilities. GPMs provide robust uncertainty metrics on the predictions they make. Uncertainty in prediction is important in materials informatics because it aids in discriminating against poor prediction accuracy in machine learning. Some machine learning models do not have straightforward ways of assessing model error \cite{Bessa2017}. 

Another benefit of GPM is that it aids in inverse design and design space visualization. GPMs can explicitly identify regions of the design space which will maximize or minimize a value. In the case of Tapia et al. response surfaces were created from the GPM which visualized the depth of melt pools as a function of laser power and speed. Doing so allows engineers to identify regions of the design space which provide specific material responses, an important tool in optimization for additive.

Another approach to full scale models in AM is building high-fidelity models from an ensemble of low-fidelity models. Current integrated computational models link phenomena across spatiotemporal scales by running many single-physics models and passing the results from model to model, then comparing with experimental results. An example is the model of Martukanitz et al. that considered the thermal, mechanical, and material response of Ti-6Al-4V alloys manufactured with powder bed processes \cite{Martukanitz2014}. Martukanitz's model uses the finite element method to model the thermal and mechanical response based on classical continuum heat transfer equations. At the same time, the Calculation of Phase Diagrams (CALPHAD) method modeled the thermodynamics and mass transfer for each chemical species, while Diffusion-Controlled phase Transformations (DICTRA) is used to model phase formation. As noted by Martukanitz, this approach becomes computationally infeasible as the number of deposition layers increases.

Even a \textit{single} modeling method -- just FEA, phase field, CALPHAD, etc -- becomes computationally expensive for thousands of layers of material deposition. Based on this scaling, accurately modeling the physics of a full build seems prohibitive, at least without a major leap in computing power. A machine learning method known as \textit{committee voting} or \textit{ensemble modeling} may provide a workaround. These methods rely on sampling many individual models to predict the behavior of a larger class of physics.

In traditional ensemble modeling approaches, many different regression models are trained to model a relationship $y = f(\mathbf{x})$, as in regression trees. For a given input $\mathbf{x_1}$ all regression models are assessed and various outputs predicted, $f_1(\mathbf{x}), f_2(\mathbf{x}), ..., f_n(\mathbf{x})$, etc. Each model $f_i(\mathbf{x})$ is a different type of machine learning regression model. Different types of regression approaches are robust for different trends in data sets. The individual regression models could be a random forest, support vector machine, neural network, or any of the type of regression models discussed herein. A linear combination of these models can have a higher prediction accuracy than any individual model.

In AM, the same concept can be extended to datasets sampled from different physical models, instead of an ensemble of different regression models. Each model $f_i(\mathbf{x}_j)$ can be trained on the inputs and outputs of the $j$th AM simulation. These data sets for additive may be the inputs and solutions of a solidification or heat transfer model, for example, or experimentally obtained relationships.

Ensemble modeling can be used in AM to solve multiobjective optimization problems, such as optimizing the heat transfer through a build and the grain growth simultaneously. Doing so would require training an ensemble model of simulations. The simulations to include could be thermomechanical models of heat transfer, phase field models of grain growth, finite element models of laser absorption, and so forth. The final ensemble model would take the form
\begin{equation}
	\mathbf{y}(\mathbf{x}) = \sum_{i=1}^N w_i f_i(\mathbf{x})
	\label{ensemble}
\end{equation}
where $\mathbf{y}$ is a response vector of properties to optimize, $f_i(\mathbf{x})$ is a single simulation of the process out of $N$, and $w_i$ are weights associated with each simulation. The vector $\mathbf{y}$ would contain many parameters such as temperature $T(t)$, temperature gradient $\nabla T$, solidification velocity $\nu(t)$, and many more phenomenon to monitor from simulations. 

In the AM case, where information from models may overlap, this type of approach can screen out noise from simulations or experiments to gauge a more fundamental relationship. The uncertainties associated with a model can be weighted by the experimentally observed data points. Furthermore, predictions can be made across many different physical models, resulting in a predictive method for holistic analysis of the final properties. This method does not provide a picture of how the physics in each simulation interact or disagree. However, it can be used to simultaneously optimize the results of many different simulations and point toward desirable manufacturing conditions. A review of multiobjective optimization functions can be found at \cite{Jin2008}.

Meredig et al. applied this method to the prediction of ABC ternary compounds, where A, B, and C each represent an element \cite{Meredig2014}. The space of all combinations of A, B, and C elements is large and would require many, many simulations. Instead of running high-throughput DFT for all possibilities they ran calculations for AB, AC, and BC compounds to generate a database of information. Then, regression trees were trained from the inputs of each binary alloy simulation to predict formation energy. Finally, an ensemble model was trained of the form
\begin{equation}
	E(\text{ABC}) = w_{\text{AB}} f(\mathbf{x}_{\text{AB}}) + w_{\text{AC}} f(\mathbf{x}_{\text{AC}}) + w_{\text{BC}} f(\mathbf{x}_{\text{BC}})
	\label{dftensemble}
\end{equation}
where $E()$ is the formation energy, $w_\text{AB}$ is the weight for a regression model of AB alloys, $f(\mathbf{x}_{\text{AB}})$ is the regression predicting formation energies for AB alloys, and so on. In the end, Meredig's model was able to predict $4,500$ new, stable ternary materials.

Machine learning is not only limited to ex situ experimental investigations or modeling approaches. Machine learning has also made major advances in signal processing and feedback. Many of the same ideas which apply to experimental and modeling studies can also be aids for in situ analysis and feedback of the manufacturing process. In particular, they can be used for simultaneous feature recognition and processing in images of AM processes.