\subsubsection{Surrogate Modeling}

If the criterion for model success is prediction accuracy, then we are not limited to physics-based computational models for data mining. Applying machine learning to a multitude of data -- empirical and computational -- can sample and predict results just as well. A \textit{surrogate model} is a mathematical approximation that can take inputs to numerical simulations and predict the results of a simulation, without actually performing the computation \textbf{\textit{or}} take the independent variables of an experiment and predict measurement values. This is done through statistical sampling of previously run models/experiments, analogous to the manner in which Kamath et al. utilized random forests, or how lower order models can be used to predict the results of higher order models. They have the added benefit of use for inverse design of materials or manufacturing conditions. 

Tapia et al. built a surrogate model for laser powder fusion of 316L stainless steel. They were concerned with predicting the melt pool depth of single-track prints solely from the laser power, velocity, and spot size \cite{Tapia2017}. Data were taken from several different sources. A few datasets were computationally derived, based on previous simulation methods used by the same research team \cite{King2014, Khairallah2016}. They ran powder bed simulations at various laser powers, velocities, and spot sizes, and the model told them the depth of the melt pool, among other information. Their dataset also included some experimentally obtained values of the same information. The combination of these datasets provided enough information for the surrogate model to search a combinatorial design space of manufacturing conditions in a wide enough scope to find local maxima of quality.

To build this model, Tapia et al. used a regression model known as a Gaussian process model (GPM). All data in a GPM is assumed to be evenly distributed in a multivariate normal manner. As with many machine learning models, trends in inputs and outputs in a GPM are elucidated through the covariance of the dataset. Starting with the covariance of their inputs, Tapia et al. used Bayesian statistics to develop a probabilistic model that predicted melt pool depth for the given input parameters. They were able to successfully predict the outcomes of both high-fidelity simulations and experimental measurements solely by analyzing trends in previously obtained results. In particular, they were able to accurately predict the melt pool depth at a value that had never been observed before, either computationally or experimentally. 