\subsubsection{Model Complexity and Dimensionality Reduction}
A bottleneck in ICME approaches is the number of different models required to simulate all physics during additive manufacturing. The spatiotemporal scales that \textit{could} be taken into consideration for AM process modeling span microns to meters. ICME of AM process modeling incorporates phase field, cellular automata, finite element, direct element methods, and more. At the smallest scale the feedstock, heat source, and melt pool dynamics have been modeled by finite element methods \cite{Toyserkani2004, Khairallah2016, Manvatkar2014} or finite  volume methods \cite{Dai2014}. Microstructure growth from the melt pool is often the next phenomenon to be studied and has been modeled through phase field \cite{Chen2002, Gong2015, Kundin2015, Sahoo2016}, cellular automata \cite{Tan2011}, or finite element methods \cite{Nie2014}. Thermal histories of entire parts or sections can be modeled next, typically by finite element methods. Thermal history models look at heat transfer through the part \cite{Michaleris2014}, residual stress build up during manufacturing \cite{Pal2014, Ding2011}, and thermal history such as cooling rate and temperature gradient \cite{Li2014, Raghavan2016}. Full ICME approaches involve modeling the AM process at all these steps to determine the entire processing history for a part. Martukanitz et al. published a full ICME investigation of AM \cite{Martukanitz2014}. A review of ICME approaches across spatiotemporal scales can be found at \cite{Francois2017} and a review of the physics of AM modeling can be found at \cite{King2015a}. A review of finite element methods specifically for AM can be found at \cite{Gouge2018}. 

For many engineering studies of AM not all of these physics \textit{need} to be modeled or included. The complexity of AM, however, obscures which physics are relevant to a given final property. When considering the design space of additive, listing out the possible parameters and properties to model illustrates the complexity of modeling. A data point in the AM process could be written as a vector 
\eqn
\begin{split}
	\mathbf{x} & =  {\large(} \text{Laser Power}, \text{Laser Speed}, \text{Laser Spot Size}, \hdots \\
			& \text{Energy Density} , \text{Alloy}, \text{Build Time}, \hdots \\
			&\text{Build Orientation},   \text{Hardness}, \text{Ultimate Tensile Strength}, \hdots \\
			& \text{Toughness}, \text{Porosity}, \text{Surface Roughness}, \hdots \large{)}. \\
\end{split}
\equ
By some estimates, the vector $\mathbf{x}$ could be 160 variables long if only accounting for tunable machine parameters. Complexity in ICME of AM only grows when considering cross-correlated information across different models. The result of one simulation $y$ could be an input $\mathbf{x}$ of another simulation. A thermomechanical model may predict temperature gradient in deposited layers, while a phase field model may use thermal gradient to calculate solidification velocity. This problem motivates using computational models which balance accuracy with computational complexity.

Dimensionality reduction algorithms identify which parameters are relevant to model in an ICME approach and which are not, informing future ICME investigations of faster simulation routes to achieve the same result. The dimensionality reduction techniques covered in this review can be broadly classified into three mathematical frameworks: statistically driven methods, similarity analysis, and matrix factorization.

Statistically driven approaches can be employed to determine the parameters in $\mathbf{x}$ that strongly impact AM model outputs, such as thermal history or materials property. A commonly used, statistically driven dimensionality reduction method is through random forest networks, as is done in design of experiments. The difference, however, is how uncertainty in the random forest is employed. In design of experiments, inputs or outputs with high associated uncertainty in the random forest model may be chosen for future tests. In dimensionality reduction, inputs which have high uncertainty are indicative of irrelevant features to model.

Kamath utilized random forests to screen out irrelevant modeling parameters for predicting maximum density of additively manufactured parts \cite{Kamath2016}. Kamath started with an experimental dataset of manufacturing parameters and multiple modeling methods. An Eagar-Tsai simulation of a Gaussian laser beam on a powder bed was used to model thermal conduction during manufacture. The model originally began with four inputs (laser power, speed, beam size, and powder absorptivity) and a design space of 462 possible input combinations. Kamath utilized random forests to determine which input was most important for achieving fully dense parts. If simulations are expensive to run then 462 different simulations may be out of the question. Identifying which parameters do not impact the final result reduces the size of input combinations, therefore reducing the number of computations to be performed. As a fictitious example, if the angle between the laser and part surface does not impact the result of a simulation, then simulations do not need to be run which vary the angle. This is valuable, time-saving information in an ICME study.

Kamath identified that laser speed and power were the most important inputs out of the four to determine melt pool depth and shape. Thus, model predictions made varying only these parameters can be relied upon for their results.  After determining the most important inputs, the same regression tree was applied in order to find optimized manufacturing conditions for fully dense parts, using the same approach. Instead of identifying which features impacted the model standard deviation, the machine settings which maximized $y$ were found. 

Dimensionality reduction through similarity analysis allows visual interpretations of distributions in high dimensional space. Understanding an $n$-dimensional distribution is difficult. Process maps are commonly employed in AM to ascertain the distribution of material properties with processing parameters \cite{Beuth2001}. Process maps are 2D plots which chart the possible values of machine inputs and identify regions of the design space with similar properties. A commonly employed process map in AM is the divide between grain morphology as a function of solidification velocity $R$ and temperature gradient $G$ \cite{DeHoff2015}. Plotting the distribution of $n$ many process variables would equate to ${n \choose 2}$ plots. Similarity analysis can be used to express distributions measured in an $n$ dimensional space in a human interpretable way without relying on multiple 2D process maps. 

The algorithms discussed thus far have used AM data `as is' by using machine inputs and materials properties to train the model. It can be equally useful to train models on \textit{correlations} between machine inputs, instead of inputs themselves. This approach combines similarity analysis with regression. Covariance between AM parameters are measured by similarity and these similarity scores are the data for a regression model. 

$t$-distributed Stochastic Neighborhood Embedding (tSNE) is a dimensionality reduction technique which measures distances in a high dimensional space and then projects data points onto a two dimensional plot. The similarity of all data points in the design space with each other can be used to fit a distribution of similarities. The tSNE algorithm begins by fitting a probability distribution to all $\mathbf{x}$'s contained in a dataset. Relationships in $n$ dimensional space are assessed through a \textit{kernel function} $\kappa(\mathbf{x},\mathbf{x'})$ which measures similarity between points in the design space. A commonly employed kernel is the Gaussian kernel
\eqn
	\kappa(\mathbf{x},\mathbf{x'}) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left[- \frac{\mathbf{x} - \mathbf{x'}}{2\sigma^2}\right]}
	\label{gausskernel}
\equ 
where $\sigma$ is a user-specified or fit standard deviation in the distribution of points in the design space. This kernel function assesses distance in the $n$ dimensional space and assigns a similarity value between $\left[0, 1\right]$ depending on how similar the values are. 

 After the $n$ dimensional dataset is fit, then a 2 dimensional coordinate $\mathbf{x}^*$ is assigned to each $\mathbf{x}$. The reason for choosing a 2 dimensional coordinate is so that the final result can be visualized on a 2D plot. The tSNE algorithm works by fitting a probability distribution to the $n$ dimensional data set first, then assigning values to each $\mathbf{x}^*$ such that they have the same probability as their associated high-dimensional $\mathbf{x}$. Once the probability distributions have been assigned, the $\mathbf{x}^*$ values can be visualized on a 2D plot to investigate for trends.

The benefit of tSNE is that points which are close together in the $n$ dimensional space appear close together on the 2 dimensional plot. This gives AM modelers an idea of how machine inputs and material behavior are distributed in the $n$ dimensional space through a 2 dimensional visualization. This dimensionality reduction technique guides modelers as to what simulation conditions will reveal similar results. Furthermore, these 2D projections provide clusters of inputs and outputs which are similar in the high dimensional space. Analysis of these clusters can inform modelers as to correlations within their inputs and outputs, identifying which parameters can be screened out to save complexity.

A final dimensionality reduction technique requires expressing model data in a matrix and performing matrix factorization. For a dataset of model inputs and results, a matrix can be formed $\mathbf{X}$ whose rows are the machine inputs, calculated properties, and simulation conditions contained in the vector $\mathbf{x}_i$. Matrix factorization techniques re-represents correlations in large datasets in a simplified way. The matrix $\mathbf{X}^T\mathbf{X}$ is a measure of covariance within $\mathbf{X}$, different than Eqn. \ref{gausskernel} but equally valid. The matrix $\mathbf{X}^T\mathbf{X}$ can be very large due to the design space of additive manufacturing. One type of matrix factorization, called Principal Component Analysis (PCA) re-represents the data matrix $\mathbf{X}$ as
\eqn
	\mathbf{X} = \mathbf{U} \mathbf{\Sigma}^T
	\label{PCA}
\equ
where the rows of $\Sigma$ are the eigenvectors of $\mathbf{X}^T\mathbf{X}$ and the diagonal entries of $\mathbf{U}$ are the square root of the eigenvalues $\sqrt{\lambda}$. PCA operates such that the first eigenvector in $\Sigma$ indicates the most heavily correlated inputs of $\mathbf{X}$. The length of vectors in $\Sigma$ is $p$ many variables long. In cases where the original design space size $n$ is large, the size of $p$ is often much less than $n$, reducing the dimension of the problem. Regression can be performed on one, a few, or many of the eigenvectors in $\Sigma$ to predict new model results using considerably less information than that contained in $\mathbf{X}$. Some studies have gone as far as to understand which correlations are being represented by the eigenvectors in $\Sigma$, revealing inputs or phenomenon which can be ignored during modeling. Such analysis is possible in AM, though requires further processing steps past matrix factorization. Materials science studies have utilized PCA previously to re-represent large datasets in simpler forms, such as predicting the formation energies of crystal structures from a lower dimensional space \cite{Curtarolo2003}. A review of applications of PCA in materials science can be found at \cite{Rajan2009}.

In additive manufacturing, PCA can serve as a dimensionally reducing pre-processing technique. For a large data set with many machine parameters the design space can be reduced from vectors of $n$ length to $p$ length. The new vectors can then be used as regression model inputs, further reducing the computational complexity of AM. 
