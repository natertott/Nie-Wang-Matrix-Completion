\subsubsection{Model Complexity and Dimensionality Reduction}
A bottleneck in ICME approaches is the number of different models required to simulate all physics during additive manufacturing. The spatiotemporal scales that \textit{could} be taken into consideration for AM process modeling span microns to meters. A full ICME approach to AM process modeling incorporates modeling efforts across phase field methods, cellular automata, finite element, direct element, and more. At the smallest scale the feedstock, heat source, and melt pool dynamics have been modeled by finite element methods \cite{Toyserkani2004, Khairallah2016, Manvatkar2014} or finite  volume methods \cite{Dai2014}. Microstructure growth from the melt pool is often the next phenomenon to be studied and has been modeled through phase field \cite{Chen2002, Gong2015, Kundin2015, Sahoo2016}, cellular automata \cite{Tan2011}, or finite element methods \cite{Nie2014}. Thermal histories of entire parts or sections can be modeled next, typically by finite element methods. Thermal history models look at heat transfer through the part \cite{Michaleris2014}, residual stress build up during manufacturing \cite{Pal2014, Ding2011}, and thermal history such as cooling rate and temperature gradient \cite{Li2014, Raghavan2016}. Full ICME approaches involve modeling the AM process at all these steps, along with experimental investigations, to determine the full processing route for an AM metal. Journal articles have also been published on full-scale ICME modeling studies of AM, such as that of Martukanitz et al. \cite{Martukanitz2014}. A review of ICME approaches across spatiotemporal scales can be found at \cite{Francois2017} and a review of the physics of AM modeling can be found at \cite{King2015a}. A review of finite element methods specifically for AM can be found at \cite{Gouge2018}. 

For many engineering studies of AM not all of these physics \textit{need} to be modeled or included. The complexity of AM, however, obscures which physics are relevant to a given final property. When considering the design space of additive, listing out the possible parameters and properties to model illustrates the complexity of modeling. A data point in the AM process could be written as a vector 
\eqn
\begin{split}
	\mathbf{x} & =  {\large(} \text{Laser Power}, \text{Laser Speed}, \text{Laser Spot Size}, \hdots \\
			& \text{Energy Density} , \text{Alloy}, \text{Build Time}, \hdots \\
			&\text{Build Orientation},   \text{Hardness}, \text{Ultimate Tensile Strength}, \hdots \\
			& \text{Toughness}, \text{Porosity}, \text{Surface Roughness}, \hdots \large{)}. \\
\end{split}
\equ
By some estimates, the vectors $\mathbf{x}$ could be 160 variables long if only accounting for tunable machine parameters. Material properties also need to be considered when searching for correlations, adding to the length of $\mathbf{x}$. Some of the machine parameters in $\mathbf{x}$ will be correlated, like laser speed and energy density. Some of the material properties will be be correlated, such as surface roughness and ultimate tensile strength. Surely, the machine settings will be correlated to material properties. The size of a data space $\mathbf{X}$ can grow quickly in additive manufacturing. This problem motivates using computational models which balance accuracy with computational complexity.

Determining which machine parameters are correlated aids in reducing computational complexity by identifying which machine parameters can be ignored during simulation. If two or more parameters are heavily correlated, then simulating one should implicitly reveal the other. 

Dimensionality reduction algorithms identify which parameters are relevant to model in an ICME approach and which are not, informing future ICME investigations of faster simulation routes to achieve the same result. The dimensionality reduction techniques covered in this review can be broadly classified into three mathematical frameworks: statistical modeling, matrix factorization, and similarity analysis.

Statistical analysis approaches can be employed to determine the parameters in $\mathbf{x}$ that strongly impact material properties. This knowledge allows designers to only model the physics impacting that property, screening out unnecessary simulations. Statistical analysis methods take many simulations of a model $f(\mathbf{x})$ and determine which parameters in $\mathbf{x}$ are most often correlated with each other or with the result $f(\mathbf{x})$. Dimensionality reduction through statistical analysis is similar to data-driven design of experiments, where statistical regression models are fit to the inputs and outputs of simulation to ascertain regions of high or low uncertainty in the design space.

Kamath utilized random forests to screen out irrelevant modeling parameters for predicting maximum density of additively manufactured parts \cite{Kamath2016}. Kamath started with an experimental dataset of manufacturing parameters and multiple modeling methods. An Eagar-Tsai simulation of a Gaussian laser beam on a powder bed was used to model thermal conduction during manufacture. The model originally began with four inputs (laser power, speed, beam size, and powder absorptivity) and a design space of 462 possible input combinations. The model was also supplemented with an experimental dataset of single-track builds. Kamath utilized random forests to determine which input was most important for achieving fully dense parts.

The main idea in Kamath's study was that variables that had a high impact would have a low standard deviation in their associated outputs as measured by the statistical (random forest) model. Using this method, they found that laser speed and power were the most important inputs out of the four to determine melt pool depth and shape. Thus, model predictions made using only these can be relied upon for their results.  After determining the most important inputs, the same regression tree was applied in order to find optimized manufacturing conditions for fully dense parts, using the same method. 

Similarity analysis is a dimensionality reduction technique that identifies similar manufacturing conditions out of a high dimensional design space. Ascertaining the distribution of machine inputs in the full AM design space is clouded by the dimension of the space. There are many possible ways to fit distributions in a space of tens of coordinates. Basic similarity metrics, like the vector norms introduced in Section II, are useful for vectors with only a few entires. When dealing with the large input and output size of AM, a vector distance approach does not accurately reflect similarities or differences between vectors containing model data. Similarity analysis such as t-distributed Stochastic Neighborhood Embedding (tSNE) measures distances in a high dimensional space and then projects data points onto a two dimensional plot. Points which are close together in the high dimensional sense appear close together on the two dimensional plot. This dimensionality reduction technique guides modelers as to what simulation conditions will reveal similar results. Furthermore, these 2D projects provide clusters of inputs and outputs which are similar in the high dimensional space. Analysis of these clusters can inform modelers as to correlations within their inputs and outputs, identifying which parameters can be screened out to save complexity.

A final dimensionality reduction technique requires expressing model data in a matrix. For a dataset of model inputs and results, a matrix can be formed $\mathbf{X}$ whose rows are the machine inputs, calculated properties, and simulation conditions contained in the vector $\mathbf{x}_i$. Matrix factorization techniques re-represents correlations in large datasets in a simplified way. The matrix $\mathbf{X}^T\mathbf{X}$ is the covariance of $\mathbf{X}$, or the amount of cross-correlated information between the model parameters. The matrix $\mathbf{X}^T\mathbf{X}$ can be very large due to the design space of additive manufacturing. One type of matrix factorization, called Principal Component Analysis (PCA) re-represents the data matrix $\mathbf{X}$ as
\eqn
	\mathbf{X} = \mathbf{U} \mathbf{\Sigma}^T
	\label{PCA}
\equ
where the rows of $\Sigma$ are the eigenvectors of $\mathbf{X}^T\mathbf{X}$ and the diagonal entries of $\mathbf{U}$ are the square root of the eigenvalues $\sqrt{\lambda}$. PCA operates such that the first eigenvector in $\Sigma$ indicates the most heavily correlated inputs of $\mathbf{X}$. The length of vectors in $\Sigma$ is $p$ many variables long. In cases where the original design space size $n$ is large, the size of $p$ is often much less than $n$, reducing the dimension of the problem. Regression can be performed on one, a few, or many of the eigenvectors in $\Sigma$ to predict new model results using considerably less information than that contained in $\mathbf{X}$. Some studies have gone as far as to understand which correlations are being represented by the eigenvectors in $\Sigma$, revealing inputs or phenomenon which can be ignored during modeling. Such analysis is possible in AM, though requires further processing steps past matrix factorization. Materials science studies has utilized PCA previously to re-represent large datasets in simpler forms, such as predicting the formation energies of crystal structures from a lower dimensional space \cite{Curtarolo2003}. A review of applications of PCA in materials science can be found at \cite{Rajan2009}.
