\subsubsection{In Situ Process Monitoring and Feedback}
\begin{figure*}
	\includegraphics[width=0.85\linewidth]{Images/melt_pool}
	\caption{A few examples of data types, data sensors, and features to detect in a laser powder bed fusion manufacturing process. The wide range of signals to monitor then control makes feedback and control in AM especially difficult. Computer vision techniques can be applied to automatically detect features of interest across multiple data types and data sensor simultaneously.}
	\label{fig:melt_pool}
\end{figure*}

In situ monitoring, feedback, and control has been consistently ranked as one of the most-needed technologies for advancing additive manufacturing \cite{Berumen2010, Tapia2014, Mani2017}. The combination of rapid solidification and the small length scales of AM solidification can make traditional process monitoring approaches difficult. Furthermore, there are many processes/problems to monitor for during the manufacturing process, with equally as many sensor types for monitoring as shown in Figure \ref{fig:melt_pool}. Machine learning can fill in gaps where human-specified process monitoring models are insufficient.

Process monitoring involves acquisition of real-time signals which can reveal information about a wide variety of phenomenon during manufacturing. Most studies utilizing in situ process monitoring are focused on identifying a) important indicators of microstructure development; or b) deviations from the normal process that cause defect formation. McKeown et al. has used dynamic transmission electron microscopy to measure solidification rates in powder bed AM \cite{McKeown2016}. Bertoli et al. has also characterized cooling rates using high speed imaging \cite{Bertoli2017}. Raplee et al. has used thermography to monitor the solidification and cooling rates of electron beam powder bed fusion, relating the temperature profiles to defect and microstructural characteristics \cite{Raplee2017}. Distortion of parts due to thermal cycling was investigated by Denlinger et al. by means of thermocouples in contact with the build substrate \cite{Denlinger2015}. 

Traditional feedback and control for manufacturing involves human-identified signatures/errors in measured signals. A basic feedback and control system is the proportional-integral-derivative (PID) controller that, in its most basic application, measures signal error from an expected mean and provides feedback to correct for deviations. Additive manufacturing has so many signals to simultaneously monitor that a desired `mean' function may not be obvious to define. A more adaptable identification and feedback method is machine learning and, specifically, computer vision. Computer vision can be employed in additive manufacturing to monitor and characterize temperature profiles, identify abnormal melt pool morphologies, and automatically detect defect formation. 

The type of data being collected in situ is often in the form of time series or image data. In computer vision, as with traditional feedback and control, algorithms are used to identify deviations from a desired signal. The power of computer vision approaches is their ability to simultaneously monitor and identify signal changes across multiple sensor types, as well as multiples different types of deviation from a single sensor. Examples include identifying a spike in temperature or a sharp change in intensity in an image indicating a deviation from a desired processing conditions. Image processing \textit{filters} can be used to selectively modify or extract features AM data. Image processing filters are mathematically analogous to those introduced for topology optimization (Section~\ref{sec:topology optimization}).

A filter is implemented as a mathematical operation, a kernel, applied to a window of time series data or an area of pixels in an image. For images, filters attempt to use local spatial information and \textit{a priori} knowledge of the expected properties of the image to improve image quality and extract features, e.g., distinctive characteristics such as edges or regions of similar intensity (domains) that represent the boundaries or spatial extents of objects, phases, etc. 

From ejected particle moving across the field of view in milliseconds to multi-hour builds and sub-millimeter melt pools to part-scale thermal distortions, AM processes span several orders-of-magnitude in both length and time scales, Practically, then, in situ monitoring requires compromises in data collection rates and resolutions, and data processing filters are used to reduce noise and extract features, such as melt pool width, from the as-collected data. A comprehensive review of image filters is beyond the scope of this review, so the interested reader is directed to the many works on this topic, c.f.,~\citet{Vernon1991}. However, three use cases are especially common and worth discussion here: reduction of high-frequency noise, also known as salt-and-pepper noise; additive noise reduction; and edge detection.

High frequency noise is characterized by sudden changes in intensity relative to the surrounding field. Although there are a number of possible causes, this may be caused by pixel-level variability or insufficiency in the detector, e.g. ``dead pixels'' or excessive gain. Median and conservative filters are commonly used when the fraction of noise pixels is large (1\%--10\%) and small ($<$ 1\%), respectively.

Additive noise, unlike high frequency noise, is a result of insufficient counting statistics, which may result from insufficient incident intensity, exposure time, or detector efficiency. A gaussian filter adjusts the intensity of each pixel according to the weighted intensities of neighboring pixels. Unlike median and conservative filters, a gaussian filter will soften edges, making adjacent domains less distinct.

Filters also have applications beyond noise reduction, primarily in object and feature detection. Detecting phenomena of interest during manufacturing is the first step to feedback and control mechanisms. Edge detection captures local changes in intensity to identify transitions between adjacent domains. Laplacian or Laplacian of Gaussian (LoG) filters themselves are sensitive to noisy images, identifying spurious edge artifacts, but are used as part of larger algorithms, such as Canny edge detection~\cite{Canny1986}. Canny edge detection include noise reduction to mitigate artifacts of LoG filters, and can be used to monitor melt pool shape and identify other features, such as unmelted powder particles attached to the build surface.

Canny edge detection, along with other feature extraction algorithms, can be used to extract the features that characterize the build and can be used as part of a larger machine learning workflow to classify build quality. For example, these features can be used in learning algorithms to correlate characteristic features, such as melt pool width and hatch spacing, with particular behaviors, such as the formation of lack of fusion defects, in the manufacturing process. In this case, identification of a feature, or set of features, may be sufficient to indicate a particular process outcome.

Template matching is a computer vision method that can be used for automatic identification of common patterns. It involves the comparison of an unclassified input to a database of pre-identified patterns. For AM, template features include abnormal melt pool morphologies, inclusion of unmelted powder particles, and denudation near the melt zone. The scale-invariant feature transform (SIFT) \cite{Lowe2004} and a variant, ``Speeded-Up Robust Features'' (SURF) \cite{Bay2008} are both feature identification algorithms that can be used for template matching. Another template matching algorithm is the \textit{bag of visual words} or dictionary method~\cite{DeCost2015}. A collection (dictionary) of typical features from the AM process can be built based on features obtained from filters. The features measured in situ are compared with dictionary entries. If an in situ feature matches a defect-indicative feature from the dictionary, then it is likely a defect has formed during manufacturing.

\begin{figure}
	\includegraphics[width=0.48\textwidth]{Images/ActivationFunctions}
	\caption{Common activation functions in artificial neural networks (NNs) that introduce nonlinearity into the NN. The sigmoid is the archetype activation function because the closed form solution for the derivative of the sigmoid, which is used during model fitting, is an excellent pedagogical tool; however, the rectified linear unit (ReLU) is, at present, the most common activation function in the hidden layers of NN. Uses for the other activation functions are provided in the text.}
	\label{fig:activation functions}
\end{figure}

Neural networks (NNs) are particularly well-suited to handle features extracted from images, or simply the images themselves. There are many references that describe neural networks in detail, c.f.~\cite{Hastie2009}, and an increasing number that address the specific challenges associated with neural networks in materials science~\cite{Bhadeshia2009}. There are several properties of NNs that are worth repeating here, however. Each layer in a NN is connected to the next layer through an affine (linear) transformation. This step stretches, scales, and skews the input vector.
\begin{equation}
	{\bold z}^{(i+1)} = \boldsymbol \theta_i^T {\bold x}^{(i)}
\end{equation}
where ${\bold z}^{(i+1)}$ is the input into the $(i+1)$ layer and ${\bold x}^{(i)}$ is the output from the previous, $i^\textrm{th}$ layer. Then, an activation function, such as those summarized in Figure~\ref{fig:activation functions}, introduces a non-linearity that warps/distorts the vector input to that layer. 

\begin{equation}
	{\bold x}^{(i+1)} = f \left( {\bold z}^{(i+1)} \right)
\end{equation}

The model hyperparameters $\mathbf{\theta}_i^T$ are regression weights that associate outputs from each layer $\mathbf{x}^{(i)}$ to subsequent layers $\mathbf{z}^{(i + 1)}$. By increasing the depth of the NN, that is, adding additional layers, and the width (number of nodes) of those layers, a NN can be used to approximate any function, making them powerful regression and classification tools~\cite{Hornik1989}. However, the general sparsity of materials data coupled to the complexity of process--structure--process relationship requires an understanding of the tradeoffs and requirements of using NNs in materials science, and in AM more specifically. Beyond the basics of model architecture, overfitting and the bias--variance tradeoff that is part of any machine learning model, a basic understanding of the role of activation functions can help to develop an intuition for the use of NN in materials and manufacturing.

An early use of NNs was in classification. The perceptron, logistic sigmoid (or simply, sigmoid), and hyperbolic tangent are all activation functions that choose between two options (0 or 1, or in the case of $\tanh$, -1 or 1). Although more mathematically complicated than the perceptron, the logistic sigmoid,
\begin{equation}
	\sigma(z) = \frac{1}{1 + e^z}
	\label{sigmaz}
\end{equation}
has the beneficial property that its derivative is an analytical expression of the sigmoid function itself, i.e.
\begin{equation}
	\sigma^\prime(z) = \sigma(z) \left( 1 - \sigma(z) \right),
	\label{sigmaprime}
\end{equation}
and simplifies model training used to fit the layer weights, $\boldsymbol \theta_i$. While a binary option may seem overly limiting, even multinomial classification can be broken down into a sequence of such binary classificiations: \textit{A} or not \textit{A}; and if not \textit{A}, then \textit{B} or not \textit{B}; and if not \textit{B}, \textit{C} or not \textit{C}; etc. However, such a serial solution will require more layers and, with more layers, longer training on larger datasets in order to fit all model hyperparameters.

In the case of a multinomial classification problem, a more simple network may be possible by using one-hot encoding. A one-hot encoding vector is defined for $N$ exclusive options: one element in the $N$-element vector is 1, all other values are 0. Rather than using multiple layers to construct the binomial ladder required to simulate a multinomial decision, the softmax activation function selects one-from-many in a single layer. Since each value in the input vector appears in the softmax exponent, even small differences in the magnitude of $z$ result in large differences in the output of this activation function; therefore one option, represented by one node or neuron in the layer, is approximately 1 and all others are nearly 0. Simplification of the network architecture by choosing activation functions that more closely resemble the nature of the problem emphasizes the importance of domain-specific knowledge in developing appropriate NN architectures.

NN are also used for regression. Unlike classification, regression requires a continuously variable response across the problem domain, and so the binary posit of the logistic sigmoid and the multinomial posit of the softmax are replaced with activation functions such as the exponential linear unit (ELU), softplus, Leaky ReLU, and most commonly, the Rectified Linear Unit, ReLU. Visual examples of these activation functions can be seen in Figure \ref{fig:activation functions}. While each behaves differently, particularly across the negative domain ($x < 0$), the simplicity and robustness of the ReLU have made it the most commonly used activation function for hidden layers in regression neural networks. 

Combining the concepts of neural networks and image processing filters, convolutional neural networks (CNNs) not only learn how to correlate features to results, they are designed to also identify the filters that extract those features. These networks require large numbers of hyperparameters, in the tens to hundreds of millions, which introduces an insurmountable training burden due to the sparsity of materials data. However, CNNs trained on natural images have demonstrated a remarkable similarity in their initial layers~\cite{Yosinski2014}. These first few layers identify basic shapes, edges, and colors that are common to many image types; a phenomenon that many groups have exploited to overcome the limitation of data sparsity through transfer learning~\cite{Ling2017a}, including specific work in the field of additive manufacturing. Yuan et al \cite{Yuan2018} were able to successfully monitor melt track width, standard deviation, and continuity of tracks in situ during laser powder bed manufacturing. Scime and Beuth trained a convolutional neural network to identify six different types of defect which are typical of laser powder bed fusion, with reasonable prediction accuracy \cite{Scime2018}. Kwon et al. classified melt pool morphologies using a neural network \cite{Kwon2018}. These studies represent only a few possible uses of CNN for in situ process monitoring of AM.
