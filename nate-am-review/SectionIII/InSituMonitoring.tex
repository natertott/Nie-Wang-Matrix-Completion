\subsubsection{In Situ Process Monitoring and Feedback}
%\begin{figure*}
%	\includegraphics[width=0.85\linewidth]{Images/melt_pool}
%	\caption{}
%	\label{}
%\end{figure*}

Thus far, in situ control in AM has been consistently ranked as one of the most-needed technologies for advancing the technology \cite{Berumen2010, Tapia2014, Mani2017}. The combination of rapid solidification and the small length scales of AM solidification can make traditional process monitoring approaches difficult. Machine learning can fill in gaps where human-specified process monitoring models are insufficient.

Process monitoring involves acquisition of real-time signals which can reveal information about a wide variety of phenomenon during manufacturing. Most studies utilizing in situ processing monitoring are focused on identifying a) important indicators of microstructure development; or b) deviations from the normal process that cause defect formation. McKeown et al. has used dynamic transmission electron microscopy to measure solidification rates in powder bed AM \cite{McKeown2016}. Bertoli et al. has also characterized cooling rates using high speed imaging \cite{Bertoli2017}. Raplee et al. has used thermography to monitor the solidification and cooling rates of electron beam powder bed fusion, relating the temperature profiles to defect and microstructural characteristics \cite{Raplee2017}. Distortion of parts due to thermal cycling was investigated by Denlinger et al. by means of thermocouples in contact with the build substrate \cite{Denlinger2015}. 

Traditional feedback and control for manufacturing involves human-identified signatures/errors in measured signals. A basic feedback and control system is the proportional-integral-derivative (PID) controller that, in its most basic application, measures signal error from an expected mean and provides feedback to correct for deviations. Additive manufacturing has so many signals to simultaneously monitor that a desired `mean' function may not be obvious to define. A more adaptable identification and feedback method is machine learning and, specifically, computer vision. Computer vision can be employed in additive manufacturing to monitor and characterize temperature profiles, identify abnormal melt pool morphologies, and automatically detect defect formation. 

The type of data being collected in situ is most often in the form of time series or image data. In computer vision, as will traditional feedback and control, algorithms are used to identify deviations from a desired signal. The power of computer vision approaches are their ability to simultaneously monitor and identify signal changes across multiple sensor types, as well as multiples different types of deviation from a single sensor. Examples include identifying a spike in temperature or a sharp change in intensity in an image indicating a deviation from a desired processing conditions. Image processing \textit{filters} can be used to selectively modify or extract features AM data. Image processing filters are mathematically analogous to those introduced for topology optimization (Section~\ref{sec:topology optimization}).

A filter is implemented as a mathematical operation, a kernel, applied to a window of time series data or an area of pixels in an image. For images, filters attempt to use local spatial information and \textit{a priori} knowledge of the expected properties of the image to improve image quality and extract features, e.g., distinctive characteristics such as edges or regions of similar intensity (domains) that represent the boundaries or spatial extents of objects, phases, etc. In AM, filters are often selected to reduce noise in the raw signal, or identify transitions between visually distinct regions, such as the transition between melt pool and powder bed.

A comprehensive review of image filters is beyond the scope of this review, so the interested reader is directed to the many works on this topic, c.f.,~\citet{Vernon1991}. However, three use cases are especially common and worth specific acknowledgement: reduction of high-frequency noise, also known as salt-and-pepper noise; additive noise reduction; and edge detection.

High frequency noise is characterized by sudden changes in intensity relative to the surrounding field. Although there are a number of possible causes, this may be caused by pixel-level variability or insufficiency in the detector, e.g. ``dead pixels'' or excessive gain. Median and conservative filters are commonly used when the fraction of noise pixels is large (1\%--10\%) and small ($<$ 1\%), respectively.

Additive noise, unlike high frequency noise, is a result of insufficient Poisson (counting) statistics, which may result from insufficient incident intensity, exposure time, or detector efficiency, random variability in the intensity of the input signal (Gaussian noise), or both. A gaussian filter adjusts the intensity of each pixel according to the gaussian-weighted intensities of neighboring pixels. Unlike median and conservative filters, a gaussian filter will soften edges, making adjacent domains less distinct.

Edge detection captures local changes in intensity to identify transitions between adjacent domains. Laplacian filters use a kernel approximation to the second derivative while the Laplacian of Gaussians (LoG) combines a laplacian kernel with a gaussian smoothing kernel. In noisy images these filters generally identify many spurious edges and are often used as part of a larger algorithm, such as Canny edge detection~\cite{Canny1986}.

The use of filters alone does not constitute machine learning, but features extracted using these filters can be used as part of the larger machine learning workflow. For example, these features can be used in learning algorithms to correlate characteristic features with particular behaviors in the manufacturing process. In this case, identification of a feature, or set of features, may be sufficient to indicate a particular process outcome.

A computer vision method called  \textit{template matching} can be used for automatic identification of signal features. Template matching involves the comparison of a measured features to a database of pre-identified signal features. The scale-invariant feature transform (SIFT) \cite{Lowe2004} and a variant, ``Speeded-Up Robust Features'' (SURF) \cite{Bay2008} are both feature identification algorithms that can be used for template matching. One such template matching algorithm is the \textit{bag of visual words} or dictionary method~\cite{DeCost2015}.%A collection (dictionary) of typical features from the AM process can be built based on features obtained from filters. The features measured in situ are compared with dictionary entries. If an in situ feature matches a defect-indicative feature from the dictionary, then it is likely a defect has formed during manufacturing.

{\color{red} Would a figure of a neural network help clarify this discussion, or is it unnecessary?}

Neural networks (NNs) are particularly well-suited to handle features extracted from images, or simply the images themselves. There are many references that describe neural networks in detail, c.f.~\cite{Hastie2009}, and an increasing number that address the specific challenges associated with neural networks in the larger space of materials science~\cite{Bhadeshia2009}. There are several properties of NNs that are worth repeating here, however. Each layer in a NN is connected to the next layer through an affine (linear) transformation. This step stretches, scales, and skews the input vector.
\begin{equation}
	{\bold z}^{(i+1)} = \boldsymbol \theta_i^T {\bold x}^{(i)}
\end{equation}
where ${\bold z}^{(i+1)}$ is the input into the $(i+1)$ layer and ${\bold x}^{(i)}$ is the output from the previous, $i^\textrm{th}$ layer. Then, an activation function, such as those summarized in Figure~\ref{fig:activation functions}, introduces a non-linearity that can warp distort the vector input to that layer.
\begin{equation}
	{\bold x}^{(i+1)} = f \left( {\bold z}^{(i+1)} \right)
\end{equation}
By increasing the depth of the NN, that is, adding additional layers, and the width (number of nodes) of those layers, a NN can be used to approximate any function~\cite{Hornik1989}. However, the general sparsity of materials data coupled to the complexity of process--structure--process relationship requires an understanding of the tradeoffs and requirements of using NNs in materials science, and in AM more specifically. Beyond the basics of model architecture, overfitting and the bias--variance tradeoff that is part of any machine learning model, a basic understanding of the role of activation functions can help to develop an intuition for the use of NN in materials and manufacturing.

One of the early uses of NNs was in classification. The perceptron, logistic sigmoid (or simply, sigmoid), and hyperbolic tangent are all activation functions that choose between two options (0 or 1, or in the case of $\tanh$, -1 or 1). Although more mathematically complicated than the perceptron, the logistic sigmoid,
\begin{equation}
	\sigma(z) = \frac{1}{1 + e^z}
	\label{sigmaz}
\end{equation}
has the beneficial property that its derivative is an analytical expression of the sigmoid function itself, i.e.
\begin{equation}
	\sigma^\prime(z) = \sigma(z) \left( 1 - \sigma(z) \right)
	\label{sigmaprime}
\end{equation}
which enables very quick calculation of the Jacobian during backpropagation and training, that is used to fit the layer weights, $\boldsymbol \theta_i$, also known as the model hyperparameters. While a binary posit may seem overly limiting, even multinomial classification can be broken down into a sequence of such binary posits: \textit{A} or not \textit{A}; and if not \textit{A}, then \textit{B} or not \textit{B}; and if not \textit{B}, \textit{C} or not \textit{C}; etc. However, such a serial solution will require deep neural networks (more hidden layers) and, with more hidden layers, longer training on larger datasets in order to % "in order to" rather than just "to" distinguishes this phrase from the use of the infinitive form "to fit".
fit all model hyperparameters.

In the case of a multinomial classification problem, a more simple network may be possible by using one-hot encoding. A one-hot encoding vector is defined for $N$ exclusive options: one element in the $N$-element vector is 1, all other values are 0. Rather than using multiple layers to construct the binomial ladder required to simulate a multinomial decision, the softmax activation function selects one-from-many in a single layer. Since each value in the input vector appears in the softmax exponent; even small differences in the magnitude of $z$ result in large differences in the output of this activation function; one option, represented by one node or neuron in the layer, dominates. Simplification of the network architecture by choosing activation functions that more closely resemble the nature of the problem is one point where domain-specific knowledge is critical.

NN are also used for regression. Unlike classification, regression requires a continuously variable response across the problem domain, and so the binary posit of the logistic sigmoid and the multinomial posit of the softmax are replaced with activation functions such as ELU, softplus, Leaky ReLU, and most commonly, ReLU, the Rectified Linear Unit. While each behaves differently, particularly across the negative domain ($x < 0$), the simplicity and robustness of the ReLU have made it the most commonly used activation function for regression-type layers.

\begin{figure*}
	\includegraphics[width=0.85\linewidth]{Images/ActivationFunctions}
	\caption{Common activation functions in artificial neural networks (NNs) that introduce nonlinearity into the NN. The sigmoid is the archetype activation function because the closed form solution for the derivative of the sigmoid, which is used in backpropagation, is an excellent pedagogical tool; however, the rectified linear unit (ReLU) is, at present, the most common activation function in the hidden layers of NN. Uses for the other activation functions are provided in the text.}
	\label{fig:activation functions}
\end{figure*}

Combining the concepts of neural networks and image processing filters, convolutional neural networks (CNNs) not only learn how to correlate features to results, they are specifically designed to identify the filters that extract those features. These networks require large numbers of hyperparameters, in the tens to hundreds of millions, which introduces an insurmountable training burden due to the sparsity of materials data. However, CNNs trained on natural images have demonstrated a remarkable similarity in their initial layers~\cite{Yosinski2014}. These first few layers identify basic shapes, edges, and colors that are common to many image types; a phenomenon that many groups have exploited overcome the limitation of data sparsity through transfer learning~\cite{Ling2017a}, including specific work in the field of additive manufacturing. Yuan et al \cite{Yuan2018} were able to successfully monitor melt track width, standard deviation, and continuity of tracks in situ during laser powder bed manufacturing. Scime and Beuth trained a convolutional neural network to identify six different types of defect which are typical of laser powder bed fusion, with fairly good prediction accuracy \cite{Scime2018}. Kwon et al. classified melt pool morphologies using a neural network \cite{Kwon2018}. These studies encompass only a few of the possibilities of in situ process monitoring for AM. 



