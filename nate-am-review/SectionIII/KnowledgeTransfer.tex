\subsubsection{Knowledge Transfer Between Single-Physics Models Through Fingerprint Descriptors}
Consider briefly the amalgamation of all empirical models, computational modeling techniques, and first-principles physics representations used to study the AM process. Empirical models can include classical materials science design principles, like the Hall-Petch relationship for grain size and strength \cite{Martin2017}. Computational modeling techniques span a wide range of approaches, including finite element models \textbf{cite}, discrete element models \textbf{cite}, phase field modeling \textbf{cite}, and more. A review on finite element methods specifically for additive can be found at \cite{Gouge2018}.

Current integrated computational models link phenomena across spatiotemporal scales by running many single-physics models and passing the results from model to model, then comparing with experimental results. An example is the model of Martukanitz et al. that considered the thermal, mechanical, and material response of Ti-6Al-4V alloys manufactured with powder bed processes \cite{Martukanitz2014}. Martukanitz's model uses the finite element method to model the thermal and mechanical response based on classical continuum heat transfer equations. At the same time, the Calculation of Phase Diagrams (CALPHAD) method to model the thermodynamics and mass transfer for each chemical species, while Diffusion-Controlled phase Transformations (DICTRA) is used to model phase formation. As noted by Martukanitz, this approach becomes computationally infeasible as the number of deposition layers increases.

Even a \textit{single} modeling method -- just FEA, phase field, CALPHAD, etc -- becomes computationally expensive for thousands of layers of material deposition. Based on this scaling, accurately predicting the final properties of a full build seems impossible, at least without a major leap in computing power. A machine learning method known as \textit{committee voting} or \textit{ensemble modeling} may provide a workaround. These methods rely on sampling many individual, sub-physics models to predict the behavior of a larger class of physics.

In traditional ensemble modeling approaches, many different regression models are trained to model a relationship $y = f(\mathbf{x})$, all trained on different subsets of the data. For a given input $\mathbf{x_1}$ all regression models are assessed and various outputs predicted, $f_1(\mathbf{x}), f_2(\mathbf{x}), ..., f_n(\mathbf{x})$ out of $n$ many models trained. These data sets for additive may be the inputs and solutions of a solidification or heat transfer model, or experimentally obtained relationships.

The ensemble method relies upon taking a vote from the results of all models, either by picking the solution that was guessed the most or taking the average of solutions, or something else. In the AM case, where models may overlap or have experimental data associated with them this type of approach can screen out noise from simulations or experiments to gauge a more fundamental relationship. The uncertainties associated with a model can be weighted by the experimentally observed data points. Furthermore, predictions can be made across many different physical models, resulting in a predictive method for holistic analysis of the final properties.

Meredig et al. applied this method to the prediction of ABC (ternary) compounds \cite{Meredig2014}. Instead of running high-throughput DFT simulations on the combinatorial space of all possible ABC compounds, they used lower order simulations. Tests were run for AB, AC, and BC compounds to predict the formation energies of ternary compounds.

The results of these simulations can then be used as fingerprint descriptors for predicting the results of more complicated interactions, as shown by Meredig et al. For example, classification and regression algorithms can be trained on previously run simulations in order to learn which physical phenomena impact the formation of certain defects. Predictive algorithms can then be used to interpret how the combination of these individual results might impact the overall physics of defect formation, such as keyhole porosity. This is done using metrics of distance, as discussed earlier. 