\section{Learning from the Past: Moving Towards Database-Driven Design of Additive Technologies}
The scientific approaches to studying additive manufacturing discussed herein -- parametric analysis, computational modeling, in situ monitoring, and the like -- produce data. The application of machine learning to these scientific approaches likewise produces data. All of this data contains a slice of the AM design space. AM researchers who have access to more data in the design space can better tailor manufacturing processes and material properties. Furthermore, machine learning algorithms trained on large datasets typically have higher prediction accuracy and a greater range of behaviors they can model. Making AM process-structure and process-property data open and accessible to the scientific public accelerates the rate at which research and engineering advances.

Databases of process-structure and process-property relationships are not a new concept in materials science. Databases like the Linus Pauling Files or International Crystal Structure Database have been widely used for materials design. However, databases of materials information are beginning to expand beyond general materials properties like thermodynamic behavior or crystal structure. Domain-specific databases are being generated from high throughput experimental and computational investigations that have occurred over the past thirty years. Studying and understanding the development of data-driven fields in materials science can aid in the development of data-driven approaches for additive manufacturing. 

Experimental high throughput investigations have been used in materials science for many decades \cite{Xiang1995}. Common deposition techniques have enough degrees of freedom to allow for continuous compositional variation within a single sample, which allows for continuous mapping of composition-structure-property relationships \cite{Long2007, Long2009, Kusne2015a}. These manufacturing methods have the same design space problem that additive has: the number of possible input combinations obscures many of the important underlying process-structure phenomenon. In cases like this, manufacturing and characterizing large catalogues of samples can lead to optimized properties faster than a theory-driven approach \cite{Ceder1998, Pilania2013}. High throughput deposition studies with chemical vapor deposition, metallorganic chemical vapor deposition, physical vapor deposition, and atomic layer deposition, among other techniques are commonplace for the manufacturing of sensors, batteries, photovoltaics, electronics, and the like \cite{Hampden-Smith1995, Gilmer1998, Mercey1999, Mitzi2001}. Furthermore, the parameters of interest in these studies can sometimes be quickly catalogued using high throughput characterization techniques like laboratory X-ray diffraction and electron probe microanalysis. 

These studies culminate in large libraries of material properties listed as a function of composition. As far back as the 1990s, data-driven algorithms were being applied to these large libraries of composition-property data. Evolutionary and genetic algorithms were trained on composition to predict stable crystal structure and material properties \cite{Deaven1995, Morris1996, Woodley1999, Stucke2003, Wolf2000}. Even neural networks, which did not have the widespread then use then that they have now, were being applied for the prediction of crystal structures based on composition \cite{Sumpter1996}. 

Modeling challenges in materials science have also been tackled using large databases with machine learning. Packages such as the Vienna Ab initio Simulation Package (VASP) have been employed for high throughput searches of stable material systems with a wide range of properties. Computational high throughput investigations have proven to closely match experiments in many regimes \cite{Curtarolo2005}. 
High throughput density functional theory (DFT) studies generate quite a bit of data and are therefore well equipped for machine learning and database-driven design. The application of high throughput DFT is widespread for design of materials with all sorts of properties including high temperature superconductors \cite{Kolmogorov2006}, lithium ion batteries \cite{Kang2006, Chen2012, Kirklin2013}, molecule design \cite{Mannodi-Kanakkithodi2016, Butler2018}, cathode materials \cite{Hautier2013}, piezoelectrics \cite{Roy2012}, ferroelectrics \cite{Bennett2012}, corrosion resistant films \cite{Ciobanu2005}, and thermoelectrics \cite{Wang2011, Yan2015}. Each of these studies, like parametric studies in additive, vary a set number of model input parameters and measure a lsmaterial property as the dependent response.

Yet many of the same modeling obstacles exist in DFT as in AM, such as a lack of transferability between models and the computational expense of large material systems. The design space problem exists here as well -- there are so many possible compositional combinations that knowing \textit{where} to look is difficult. Machine learning was proposed as a solution for obstacles in high throughput DFT as early as 2005 \cite{Morgan2005}.  Large unit cells whose properties cannot be directly calculated using DFT are often approximated using machine learning approaches like neural networks \cite{Behler2015}, genetic algorithms \cite{Hart2005}, and principal component analysis \cite{Snyder2012}. Studies applying machine learning to databases of computational information have gone beyond tackling computational problems. In some cases, the studies have revealed previously unobserved or uncharacterized relationships between crystal structure information and materials properties \cite{Ghiringhelli2015}. 

Image processing databases have developed outside of materials science specifically, though a database of materials-specific images is highly desirable for the training of recognition algorithms. The collection and distribution of image databases have enabled rapid developments in the field of computer vision. Many of the more common objectives with computer vision -- autonomous navigation, face recognition, object recognition, image segementation -- have databases which are catalogued in online repositories like CVonline \cite{CVonline} and VisionScience \cite{VisionScience}. Open sharing of additive microstructures will aid in the development of segmentation and identification algorithms which are suited for additive specific problems. 

In an effort to reduce the design time on material systems, programs like the Materials Project incorporate data taken from a wide range of experimental and computational methods into an open-source, accessible database. The Materials Project also features electronic, structural, and thermodynamic calculations of different materials as well as an automated workflow for doing DFT computations of material systems \cite{Jain2011, Jain2013}. Other databases of materials information include AFLOWLib \cite{Curtarolo2012, Curtarolo2012a}, the Harvard Clean Energy Project \cite{Hachmann2011}, Japan's National Institute of Material Science \cite{NIMS}, and the Open Quantum Materials Database \cite{Saal2013}. Some pipelines for high-throughput computation and analysis have included consideration of publication timelines in their processes \cite{Foster2015}. These databases offer a multitude of benefits to materials researchers. First and foremost, publicly accessible databases offer an infrastructure for the free flow of experimental and computational results. Synergy between research groups becomes easier as data is shared more freely.

Furthermore, many of these online databases also provide tools for performing material design. The Materials Project offers a design interface, whereby users can specify a set of material properties and are provided with a list of likely candidate materials. Other projects, like AFLOW, allow for fast high-throughput DFT calculations of a wide range of material systems. 

The generation of databases that are accessible to the scientific public is a primary step on the roadmap of the Materials Genome Initiative \cite{DePablo2014}. Much of the development of materials databases have focused on computationally-derived materials information. Infrastructure and standards need to be developed that allow for sharing of experimental data that is understandable and usable by many researchers. Data journals are becoming more common for sharing datasets from scientific investigations and are making strides in standardizing data-sharing infrastructure \cite{Wilkinson2016}, along with the publication of datasets themselves for public use \cite{DeJong2015, Kim2017}. 

Having open, accessible databases improves the rate at which machine learning can be applied to design for additive manfuacturing. Machine learning as a tool driving materials design was proposed some time ago. Review articles have explored the many and varied uses of machine learning across materials science, with many of the applications finding great success \cite{Kalidindi2016, Ramprasad2017, Gubernatis2018}. A review article on best practices for machine learning in materials science can be found in \cite{Wagner2016}. 

Additive manufacturing should move toward the same types of infrastructure for open data sharing. The combinatorial problems in additive are widespread and cover many, many length scales. Large institutions may have the resources to link time- and length-scales in additive manufacturing. Smaller research groups are often limited to studying a single process phenomenon and do not necessarily have means to integrate their knowledge into other additive manufacturing studies. The generation of additive databases allows for a democratization of research and an acceleration of the pace at which additive manufacturing advances are made. 