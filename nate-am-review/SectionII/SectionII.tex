\section{Phrasing Additive Manufacturing as a Machine Learning Problem}\label{phrasing}
Data-driven modeling and machine learning have been employed to great success across several fields including materials science. While machine learning may seem abstract at first, it can be expressed and understood in plain terms. Many of the tenets and frameworks for machine learning are based in mathematical operations which may be familiar to the reader, but applied in new ways. In this section, we proceed to define the basic terminology and classes of machine learning and data. Specific ML algorithms are then introduced specific to contextual AM examples in Section III.


\input{SectionII/DesignSpace}



%%%
\begin{table*}
\caption{Several of the most widely used machine learning algorithms that have been used in materials science.} \label{ML}
\begin{tabular}{|p{2.25cm}|p{2.25cm}|p{3cm}|p{4cm}|p{4cm}|}\hline
\raggedright Class of Algorithm & Examples & Applications & Strengths & Constraints \\ \hline \hline
Weighted neighborhood clustering & Decision trees, k-Nearest neighbor & \raggedright Regression, Classification, Clustering and similarity & These algorithms are robust against uncertainty in data sets and can provide intuitive relationships between inputs and outputs. See Ref. \cite{Quinlan1986} for a primer on clustering. & They can be susceptible to classification bias toward descriptors with more data entries than others. \\ \hline

\raggedright Nonlinear dimensionality reduction & \raggedright t-SNE, Kernel ridge regression, Multidimensional metric scaling &\raggedright  Dimensionality reduction, Clustering and similarity, Input/output visualization, Descriptor analysis, Regression &\raggedright These algorithms are robust against nonlinear input/output relationships and can provide intuitive projections of the material input/output space. For accessible examples, see Refs. \cite{Tenenbaum2000, Roweis2000}. & Projections can represent unphysical, difficult to interpret relationships. Global relationships can also be lost when nonlinear dimensionality reduction results are projected onto lower-dimensional spaces.\\ \hline

\raggedright Linear dimensionality reduction & \raggedright Principle component analysis (PCA), Support vector regression (SVR), Nonnegative Matrix Factorization (NMF) & Dimensionality reduction, Clustering and similarity, Input/output visualization, Descriptor analysis, Regression & \raggedright This type of algorithm can produce orthogonal basis sets which reproduce the training data space. They can also provide quick and accurate regression analysis. For a primer on PCA specifically, see Ref. \cite{Bro2014}. & The relationships studied must be linear in nature, and these algorithms are susceptible to bias when descriptors are scaled differently. \\ \hline

Search algorithms & \raggedright Genetic algorithms, Evolutionary algorithms & Alloy design (in conjunction with a material modeling approach) &  \raggedright Search algorithms are intuitive for material properties that can be described geometrically, such as topology optimization for weight reduction. They are efficient at searching spaces with multiple local extrema, such as finding local maxima of quality in multidimensional design spaces. &  These algorithms are highly dependent upon selection and mutation criteria. For a useful application of genetic algorithms to process characterization, see Ref. \cite{Grefenstette1986}. \\ \hline

Neural Networks & \raggedright Artifical Neural Networks, Convolutional Neural Networks, General Adversarial Networks & Classification, regression, feature identification and extraction in images, simulation of atomic potentials, transfer learning, in situ process monitoring, feedback and control & Neural networks have found extraordinary success in the processing and analysis of image type data; the research and development surrounding NNs is among the most mature of any type of machine learning algorithm & Neural networks tend to require extremely large datasets to be successful, especially for image analysis applications, however transfer learning approaches have been successful for adopting NNs in the analysis of small datasets \\ \hline

\end{tabular}
\end{table*}
%%%

\input{SectionII/DataTypes}
\input{SectionII/Assumptions}
\input{SectionII/Unsupervised}
\input{SectionII/Supervised}
\input{SectionII/Tools}



