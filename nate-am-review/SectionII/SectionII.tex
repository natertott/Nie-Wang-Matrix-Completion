\section{Phrasing Additive Manufacturing as a Machine Learning Problem}\label{phrasing}
Data-driven modeling and machine learning have been employed to great success across several fields including materials science. While machine learning may seem foreign at first, readers will see that it can be expressed and understood in plain terms. Many of the tenets and frameworks for machine learning are based in mathematical operations which may be familiar to the reader, but applied in new ways. 


\input{SectionII/DesignSpace}
\input{SectionII/DataTypes}
\input{SectionII/Assumptions}
\input{SectionII/Unsupervised}
\input{SectionII/Supervised}
\input{SectionII/Tools}

\begin{table*}
\caption{Several of the most widely used machine learning algorithms that have been used in materials science are compared.} \label{ML}
\begin{tabular}{p{2.25cm}|p{2.25cm}|p{3cm}|p{4cm}|p{4cm}}
\raggedright Class of Algorithm & Examples & Applications & Strengths & Constraints \\ \hline \hline
Weighted neighborhood clustering & Decision trees, k-Nearest neighbor & \raggedright Regression, Classification, Clustering and similarity & These algorithms are robust against uncertainty in data sets and can provide intuitive relationships between inputs and outputs. See Ref. \cite{Quinlan1986} for a primer on clustering. & They can be susceptible to classification bias toward descriptors with more data entries than others. \\ \hline

\raggedright Nonlinear dimensionality reduction & \raggedright t-SNE, Kernel ridge regression, Multidimensional metric scaling &\raggedright  Dimensionality reduction, Clustering and similarity, Input/output visualization, Descriptor analysis, Regression, Predictive modeling &\raggedright These algorithms are robust against nonlinear input/output relationships and can provide intuitive projections of the material input/output space. For accessible examples, see Refs. \cite{Tenenbaum2000, Roweis2000}. & Projections can represent unphysical, difficult to interpret relationships. Global relationships can also be lost when nonlinear dimensionality reduction results are projected onto lower-dimensional spaces.\\ \hline

\raggedright Linear dimensionality reduction & \raggedright Principle component analysis (PCA), Support vector regression (SVR) & Dimensionality reduction, Clustering and similarity, Input/output visualization, Descriptor analysis, Regression, Predictive modeling & \raggedright This type of algorithm can produce orthogonal basis sets which reproduce the training data space. They can also provide quick and accurate regression analysis. For a primer on PCA specifically, see Ref. \cite{Bro2014}. & The relationships studied must be linear in nature, and these algorithms are susceptible to bias when descriptors are scaled differently. \\ \hline

Search algorithms & \raggedright Genetic algorithms, Evolutionary algorithms & Searching a material space to optimize on a certain condition, Lowest-energy state searches, Crystal structure prediction &  \raggedright Search algorithms are intuitive for material properties that can be described geometrically, such as topology optimization for weight reduction. They are efficient at searching spaces with multiple local extrema, such as finding local maxima of quality in multidimensional design spaces. &  These algorithms are highly dependent upon selection and mutation criteria. For a useful application of genetic algorithms to process characterization, see Ref. \cite{Grefenstette1986}. \\

\end{tabular}
\end{table*}
%%%

