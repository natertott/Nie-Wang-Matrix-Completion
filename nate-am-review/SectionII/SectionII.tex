\section{Phrasing Additive Manufacturing as a Machine Learning Problem}\label{phrasing}

\subsection{The Assumptions Behind Machine Learning}
Ultimately, AM users desire a model that relates manufacturing parameters to AM material properties; i.e., a process-property model. In a functional form, this model can be written as
\eqn
f(x_1, x_2, ... , x_n) = \mathbf{y},
\label{fundamentalgoal}
\equ
where the inputs $x_i$ are $n$ different manufacturing parameters and the outputs $\mathbf{y}$ are measurable properties. 

Currently, these process-property relationships are often developed by connecting separate process-structure and structure-property models of individual physical processes within AM. For example, individual process-structure models are developed to relate: heat source parameters to melt pool topologies \cite{Khairallah2016} using the finite element method or similar computational techniques; melt pool topologies to solidification routes using thermal- and mass-diffusion models  \cite{Tan2011}; solidification to microstructure evolution using phase field methods \cite{Kundin2015}. Separately, structure-property models are developed such as relating grain orientations and stress to material properties using crystal plasticity \cite{Pal2014}. It is then by making connections between individual process-structure and structure-property models that researchers  relate process parameters to material properties. This understanding-through-sequential-modeling approach compounds the errors and uncertainty of each approach on one another, so that the final result is less accurate, more time consuming, and more expensive than the direct build-break approach to develop process-property relationships.

Machine learning is not a replacement for traditional approaches \textbf{what approaches are you referring to? Do you need to specify them?}, but rather a complementary modeling approach that can accelerate or even automate that process of building connections across many degrees of freedom that span the many different individual phenomenon within AM processes. Two assumptions are necessary when using machine learning:
\begin{enumerate}
\item \textit{The Relational Hypothesis}: A correlative relationship exists between the data input to the model and the response of the system.
\item \textit{The Locality Hypothesis}: Different parts manufactured at similar points in the design space will have similar properties.
\end{enumerate}

Both are required for classification and regression problems.

\subsection{Unsupervised Machine Learning}\label{unsupervised}

Unsupervised machine learning algorithms are used to identify similarities and relationships from unlabeled data. Consider an experiment that varies three different manufacturing inputs $x_1, x_2, x_3$ and measures a single material property $y$. In matrix form, the data are expressed as:
\eqn
\begin{split} 
\mathbf{X} &= \begin{bmatrix}
	x_{1,1} & x_{1,2} & x_{1,3} \\
	x_{2,1} & x_{2,2} & x_{2,3} \\
	\vdots & \vdots & \vdots \\
	x_{m,1} & x_{ m,2} & x_{m,3} \\
	\end{bmatrix} \\
\mathbf{Y} &= \begin{bmatrix}
	y_1 \\
	y_2 \\
	\vdots \\
	y_m \\
	\end{bmatrix} \\
\end{split}\label{initialmeasure}
\equ
where $x_{i,j}$ is the $j^{th}$ measurement of the $i^{th}$ manufacturing input. A \textit{distance metric} can be defined between data points in the design space. Distance metrics assess the similarity of objects like vectors. One example of the distance between two vectors $\mathbf{x}$ and $\mathbf{y}$ is the $\ell_1$ norm, given by
\begin{equation}
	||\mathbf{x} - \mathbf{y}||_1 = \mathbf{x} - \mathbf{y} 
	\label{ell1}
\end{equation}
which is just the difference of the two vectors. Another common distance metric is the $\ell_2$ norm, given by
\begin{equation}
	||\mathbf{x} - \mathbf{y}||_2^2 = \mathbf{x}^2 - \mathbf{y}^2
	\label{ell2}
\end{equation}
which is the Euclidean distance, or the straight-line distance. Distance metrics such as these provide an idea of how close two objects are in space. They can also be interpreted as the similarity of two lists of parameters. For example, data can be collected at two points $\mathbf{a} = (x_{1}, x_{2}, x_{3})$ and $\mathbf{b} = (x_{1} + \delta, x_{2}, x_{3})$ and treat these quantities as vectors. Computing the $\ell _2$ norm of $\mathbf{a}-\mathbf{b}$ yields
\eqn
|| \mathbf{a} - \mathbf{b}||_2 = \delta.
\equ
The value and magnitude of $\delta$ gives an inclination about how similar $\mathbf{a}$ and $\mathbf{b}$ are. If $\delta$ is close to zero, then a researcher can say that they are similar, or even the same if $\delta$ is exactly zero. As $\delta$ becomes larger a researcher can say $\mathbf{a}$ and $\mathbf{b}$ become more dissimilar.The concept of `similar' manufacturing conditions may be easy to assess by an experimentalist when tuning only a few parameters at a time. When taking into consideration tens or hundreds of design criteria, sometimes with correlated inputs, elucidating similar manufacturing conditions becomes difficult. This vector distance approach is a simple, yet effective first glance at similarity in a design space and is generalizable to $n$ many design criteria. 

Let us say that $\delta$ is small and that $\mathbf{a}$ and $\mathbf{b}$ are similar manufacturing conditions. Now, consider a third point in the design space $\mathbf{c} = (x_{1} + \delta, x_2 + \delta, x_3)$ that has not yet been measured. Since $\mathbf{c}$ was manufactured at similar conditions to $\mathbf{a}$, as measured by $||\mathbf{c} - \mathbf{a}||_2 = \sqrt{2}\delta$, then we may say that $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$ are all similar to each other. If the locality hypothesis is correct then manufacturing with conditions $\mathbf{a}$, $\mathbf{b}$ and $\mathbf{c}$ should yield similar measurements of $y$. 

At some point, a researcher will have a set of initial manufacturing inputs $\mathbf{a}$, $\mathbf{b}$, $\mathbf{c}$, $\mathbf{d}$, etc., and associated property measurements that have been tested. Churning through the remainder of all possible manufacturing conditions becomes expensive and tedious quickly. Instead, researchers can use similarity metrics to determine whether or not a future test is worth running. Comparing the manufacturing inputs through vector distance gives a rough idea of the possible outcome before spending time and resources on running a test. If the intent is exploring design spaces then manufacturing at conditions \textit{furthest away} from previously observed points may be the answer. If looking for local maxima of quality, an operator would want to manufacture at conditions \textit{nearest to} the conditions currently known to have high quality. 

Using vector distances as metrics of similarities can produce results that are analogous to creating process maps \textbf{find citation}. Process maps are used to divide $2$ dimensional plots of manufacturing inputs into regions of quality, or regions of different material responses. The following demonstration is based on $k$-means clustering, a commonly used unsupervised machine learning clustering algorithm.

A researcher has acquired the datasets in Eqn. \ref{initialmeasure} and wants to partition $\mathbf{Y}$ into groupings of high quality parts and low quality parts. However, there are several values of $y \in \mathbf{Y}$ that lie between two extremes and the cutoff for quality is not well defined. It would be useful to use similarity metrics to find the best possible partition of quality. To begin, the data set is partitioned randomly into two groups, $\mathbf{Y}_1$ and $\mathbf{Y}_2$. The centroids $m_1$, $m_2$ (or centers of mass, in engineering) of each grouping can be calculated as
\eqn
	\begin{split}
		m_1 & = \frac{1}{|\mathbf{Y}_1|} \sum_{y_j \in \mathbf{Y}_1} y_j \\
		m_2 & = \frac{1}{|\mathbf{Y}_2|} \sum_{y_j \in \mathbf{Y}_2} y_j. \\
		\label{moment}
	\end{split}
\equ
where $|\mathbf{Y}|$ is the average value of a grouping. The measurements were randomly partitioned at first; the goal is to re-partition each set so that similar measurements (similar levels of quality) are in the same set. To do this, we can re-assign each set by
\eqn
	\begin{split}
		\mathbf{Y}_1 & = \{y_i : ||y_i - m_1||_2 \leq ||y_i - m_2||_2 \} \\
		\mathbf{Y}_2 & = \{y_j : ||y_j - m_2||_2 \leq ||y_j - m_1||_2 \}. \\
	\end{split}
	\label{reassign}
\equ
We can interpret the re-assignment in Eqn. \ref{reassign} physically: if a measurement initially assigned to set $\mathbf{Y}_1$ is closer in distance to $\mathbf{Y}_2$ then it is \textit{more similar} to the other set. Thus, it is re-assigned. Since the original partition was random it is likely that there are low quality parts mixed in with high quality parts - in other words, outliers exist in each partition. Measuring the similarity of each data point to the mean of the groupings re-classifies these outliers into groupings that are more reflective of their quality.

Once re-assignment is complete the centroids in Eqn. \ref{moment} can be re-calculated and updated. Then, data points are re-assigned once more based on how similar they are to the centroid of each partition. If we have partitioned the input settings $(x_1, x_2, x_3)$ along with their corresponding measurements, then we have lists of input settings which are likely to give good/bad quality parts. Further analysis can also be conducted, such as analyzing which regimes of inputs lead to good or bad quality - this is precisely what process maps represent. The difference in this case is that $n$ many manufacturing conditions can be related to a quality metric simultaneously, with little to no human inspection or intervention. Additionally, a researcher can dig further and analyze \textit{why} groups of input settings result in given quality for a material property.

\subsection{Supervised Machine Learning}
In a \textit{supervised machine learning algorithm} the goal is to find a relationship $T: \mathbf{X} \to \mathbf{Y}$ that best approximates the underlying physical relationship $f(\mathbf{x}) = \mathbf{y}$. That is, supervised machine learning algorithms relate model inputs to labeled data. It is not necessary to make any assumptions up front about the nature of $f(\mathbf{x}) = \mathbf{y}$ to find $T$. We only need to rely on the relational hypothesis and assume the relationship exists. The process of finding $T$ can take many forms, depending on the specific supervised ML algorithm being used.

One method to find $T$ is assume it is a vector that satisfies
\eqn
\mathbf{X}T = \mathbf{Y}.
\label{map}
\equ
A researcher seeks this relationship through the measurements they have observed; in this case, the measurements form the design matrix, Eqn. \ref{initialmeasure}. A common method to find a vector representation of $T$, and a critical element in most machine learning algorithms, is through least squares regression. Least squares regression finds $T$ through a minimization problem, given by
\eqn
\min || \mathbf{X}T - \mathbf{Y} ||_{2}^{2}.
\label{leastsquares}
\equ
Equation \ref{leastsquares} can be interpreted analogously to similarity measurements for unsupervised algorithms: the closer that $\mathbf{X}T - \mathbf{Y}$ is to zero, the more similar $T$ is to $f(\mathbf{x})$. 

%\textbf{This next example was specifically requested by a co-author for an explanation of how LSQ is done. I think it detracts from the conversation.}\newline

%In many cases the relationship in Eqn. \ref{map} is going to be over-determined (or \textit{rank deficient}) because a researcher will have more measurements than input parameters. There will be more rows of Eqn. \ref{initialmeasure} than columns. This means that a unique solution does not necessarily exist. $\mathbf{QR}$ decomposition is commonly employed to solve the rank deficiency of Eqn. \ref{map}. $\mathbf{QR}$ decomposition involves finding a representation of $\mathbf{X}$ as the product of two matrices, $\mathbf{Q}$ and $\mathbf{R}$. The columns of $\mathbf{Q}$ represent the basis vectors of the design space that $\mathbf{X}$ inhabits. $\mathbf{R}$ is an upper-triangular matrix whose entries are the coordinates of the data points contained in $\mathbf{X}$. This representation of $\mathbf{X}$ has two advantages. First, it decomposes the manufacturing inputs contained in the rows of $\mathbf{X}$ into basis vectors with coordinates, an easily interpretable representation. Second, the nature of $\mathbf{QR}$ decomposition ensures that $\mathbf{R}$ is a square matrix. Thus, any equation involving $\mathbf{R}$ will have exactly as many variables as it has unknowns, removing the over-determinedness of the problem \cite{Poole2011}.

%Using the $\mathbf{QR}$ decomposition transforms the problem stated in Eqn. \ref{map} into
%\eqn
%\mathbf{QR}T = \mathbf{y}
%\label{QR}
%\equ
%Since $\mathbf{Q}$ is orthogonal, and therefore invertible, we can now solve the problem
%\eqn
%\mathbf{R}T = \mathbf{Q^{-1}} \mathbf{y}.
%\equ

The method of solving equation \ref{leastsquares} are many and varied; indeed, much of this review will focus on applying Eqn. \ref{leastsquares} to various problems through additive manufacturing. The result is an approximation to the functional relationship $f(\mathbf{x}) = \mathbf{y}$. A new point of interest in the design space $\mathbf{x'}$ can be chosen and its associated material property $\mathbf{y'}$ can be predicted by computing
\eqn
\mathbf{x'}T = \mathbf{y'}.
\equ

This simple example demonstrates how functional relationships can elucidate more information about design spaces. Commonly used machine learning algorithms in materials science and engineering are given in Table \ref{ML}. Note that the field of Machine Learning is evolving as fast as AM itself; hence Table \ref{ML} is by no means a comprehensive review of all machine learning algorithms, but rather is intended to provide a comparison between the form and function of some of the most widely adopted algorithms in materials science and engineering. 



%~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~_~
\subsection{Tools of the Trade}
Based on the previous discussion and examples, researchers are equipped with two general tools to integrate into ICME approaches, namely: 
\begin{itemize}
	\item Classification
	\item Prediction
\end{itemize}
These are the basic tools that will be used in the subsequent sections. The method by which similarity is measured or $f(\cdot)$ is found is dependent upon the task at hand, the data being used, and the machine learning approach chosen. In most cases, multiple appropriate approaches exist. 

Throughout the rest of the article these two tools will be built upon and applied to a plethora of problems in AM. In most cases, the discussion will be kept general - that is, the reader will be left to determine the best distance metric or regression tool to use. In cases where specific methodologies already exist for a given problem the discussion will be more focused and exact.

\subsection{Selection Criteria}
\textbf{Chris's discussion on available tools for AM go here}

%%%
\begin{table*}[t]
\caption{Several of the most widely used machine learning algorithms that have been used in materials science are compared.} \label{ML}
\begin{tabular}{p{2.25cm}|p{2.25cm}|p{3cm}|p{4cm}|p{4cm}}
\raggedright Class of Algorithm & Examples & Applications & Strengths & Constraints \\ \hline \hline
Weighted neighborhood clustering & Decision trees, k-Nearest neighbor & \raggedright Regression, Classification, Clustering and similarity & These algorithms are robust against uncertainty in data sets and can provide intuitive relationships between inputs and outputs. See Ref. \cite{Quinlan1986} for a primer on clustering. & They can be susceptible to classification bias toward descriptors with more data entries than others. \\ \hline

\raggedright Nonlinear dimensionality reduction & \raggedright t-SNE, Kernel ridge regression, Multidimensional metric scaling &\raggedright  Dimensionality reduction, Clustering and similarity, Input/output visualization, Descriptor analysis, Regression, Predictive modeling &\raggedright These algorithms are robust against nonlinear input/output relationships and can provide intuitive projections of the material input/output space. For accessible examples, see Refs. \cite{Tenenbaum2000, Roweis2000}. & Projections can represent unphysical, difficult to interpret relationships. Global relationships can also be lost when nonlinear dimensionality reduction results are projected onto lower-dimensional spaces.\\ \hline

\raggedright Linear dimensionality reduction & \raggedright Principle component analysis (PCA), Support vector regression (SVR) & Dimensionality reduction, Clustering and similarity, Input/output visualization, Descriptor analysis, Regression, Predictive modeling & \raggedright This type of algorithm can produce orthogonal basis sets which reproduce the training data space. They can also provide quick and accurate regression analysis. For a primer on PCA specifically, see Ref. \cite{Bro2014}. & The relationships studied must be linear in nature, and these algorithms are susceptible to bias when descriptors are scaled differently. \\ \hline

Search algorithms & \raggedright Genetic algorithms, Evolutionary algorithms & Searching a material space to optimize on a certain condition, Lowest-energy state searches, Crystal structure prediction &  \raggedright Search algorithms are intuitive for material properties that can be described geometrically, such as topology optimization for weight reduction. They are efficient at searching spaces with multiple local extrema, such as finding local maxima of quality in multidimensional design spaces. &  These algorithms are highly dependent upon selection and mutation criteria. For a useful application of genetic algorithms to process characterization, see Ref. \cite{Grefenstette1986}. \\ 

\end{tabular}
\end{table*}