\paragraph{Phrasing Additive Manufacturing as a Machine Learning Problem}\label{phrasing}
While machine learning may seem abstract at first, it can be expressed and understood in plain terms. Many of the tenets and frameworks for machine learning are based in mathematical operations that are likely familiar to any scientist or engineer, but applied in new ways. In this section, we proceed to define the basic terminology and classes of machine learning and data. A list of machine learning algorithms used in the papers cited in this review can be found in Table \ref{ML}. The following section details general terminology and intuition for the application of AM. Specific ML algorithms are then introduced specific to contextual AM examples in Section III.

Machine learning algorithms are mathematical constructs that may be used as scientific and/or engineering tools when warranted. They are not appropriate for all science and engineering problems - just as finite element simulations should not be used to study the mechanics of discrete interfaces or single atomic bonds between two atoms or DFT should not be used to simulate mm-sized polycrystals, machine learning algorithms should not be used to model data that lack statistical correlations. Hence, the first questions every scientist and engineer considering the use of machine learning approaches should ask and answer is: "how are the data statistically distributed?", and "are there statistical correlations between the data features of interest?" Once this is complete, then a researcher can decide if ML is appropriate.

If the data lack clear statistical correlations using basic probability analyses, machine learning is not a "magic box" that can suddenly make such correlations evident. Similarly, if the statistical distributions of the data are featureless except for an occasional outlier, machine learning cannot meaningfully fit a model that is based on statistical distributions. 

Today, many scientists and engineers are embracing the approach that "we will machine learn it," without understanding how to evaluate if machine learning is an appropriate tool to apply to a problem or not. One unpublished example in AM of a problem that ML is not well suited for is building a model to predict the location of a maximum pore within a powder bed laser fusion build. A maximum pore is a statistical outlier - usually one of thousands-to-millions, depending on the size of the part being built. Even though the pore may occur in the exact same position of the build volume if the same part is built over-and-over again (i.e., it is highly repeatable), the fact that it is a statistical anomaly means that nearly all machine learning algorithms are built to ignore it. Once this understanding is at hand, then a researcher can decide if ML is appropriate.

Still, most data of interest in metals AM have strong statistical features, as we will proceed to discuss in more detail in the examples given in this review. Once some basic statistical analysis of the data of interest has been performed and it has been determined that there are quantifiable correlations between the inputs and outputs, or across different inputs, and that there are also statistical features that describe the distributions of the data, then a researcher can proceed to consider data featurization and processing, and then tune and evaluate the performances of machine learning models to find the best performers. We proceed to describe these techniques in more detail, after defining some basic terminology used in this article.


\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/DesignSpace}

%%%
\begin{landscape}
\begin{longtable}{p{2.5cm}p{3cm}p{4cm}p{5.5cm}p{5.5cm}}
\endfirsthead

\hline
\raggedright Class of Algorithm & Examples & Applications & Strengths & Constraints \\ \hline 
Weighted neighborhood clustering & \raggedright Decision trees, Random Forest, k-Nearest neighbor & \raggedright Regression, Classification, Clustering and similarity & \raggedright These algorithms are robust against uncertainty in data sets and can provide intuitive relationships between inputs and outputs. See Ref. \cite{Quinlan1986} for a primer on clustering. & They can be susceptible to classification bias toward descriptors with more data entries than others. \\ 

\endhead
\caption{Several of the most widely used machine learning algorithms in materials science.\label{ML} }\\


\dashedrule{.1em}{.1em}{325} & & & & \\

\raggedright Linear dimensionality reduction & \raggedright Principle component analysis (PCA), Support vector regression (SVR), Nonnegative Matrix Factorization (NMF) & \raggedright Experimental design, model dimensionality reduction, model or experimental input/output visualization, descriptor analysis, regression  & \raggedright This type of algorithm can produce orthogonal basis sets that reproduce the training data space. They can also provide quick and accurate regression analysis. For a primer on PCA specifically, see Ref. \cite{Bro2014}. & The relationships studied must be linear in nature, and these algorithms are susceptible to bias when descriptors are scaled differently. \\ 

\dashedrule{.1em}{.1em}{325} & & & & \\

\raggedright Nonlinear dimensionality reduction & \raggedright t-SNE, Kernel ridge regression, Multidimensional metric scaling &\raggedright  Experimental design, model dimensionality reduction, model or experimental input/output visualization, descriptor analysis, regression &\raggedright These algorithms are robust against nonlinear input/output relationships and can help visualize similarity in high dimensional relationships. For accessible examples, see Refs. \cite{Tenenbaum2000, Roweis2000}. & Interpretation of high dimensional similarity can be difficult; while these algorithms are useful for visualizing relationships interpreting the \textit{why} of the relationship found is difficult. Global relationships can also be lost when nonlinear dimensionality reduction results are projected onto lower-dimensional spaces.\\ 

\dashedrule{.1em}{.1em}{325} & & & & \\

Search algorithms & \raggedright Genetic algorithms (GA), Evolutionary algorithms & Alloy design (in conjunction with a material modeling approach), model optimization. topology optimization for AM &  \raggedright Search algorithms are intuitive for material properties that can be described geometrically, such as topology optimization for weight reduction. They are efficient at searching spaces with multiple local extrema, such as finding local maxima of quality in multidimensional design spaces. For a useful application of genetic algorithms to process characterization, see Ref \cite{Grefenstette1986}. &  These success of these algorithms are highly dependent upon selection and mutation criteria. \\ 

\dashedrule{.1em}{.1em}{325} & & & & \\

\raggedright Neural Networks \& Computer Vision & \raggedright Artifical Neural Networks, Convolutional Neural Networks (CNN), General Adversarial Networks (GAN) & \raggedright Classification, regression, feature identification and extraction in images, simulation of atomic potentials, transfer learning, in situ process monitoring, feedback and control & Neural networks have successfully modeled processing and image data; the research and development surrounding NNs is among the most mature of any type of machine learning algorithm. & Neural networks tend to require large training datasets, especially for image analysis applications; however, transfer learning approaches can adopt NNs to small datasets. \\ \hline
\end{longtable}
\end{landscape}
%%%


\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/Sources}
\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/Featurization}
\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/Assumptions}
\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/Unsupervised}
\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/Supervised}
\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/ErrorMetrics}
\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/BiasVariance}
\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/ModelComparison}
\input{/Users/njohnson/git/thesis/document/chapters/review/SectionII/Tools}



