\subsubsection{Machine Learning as Numerical Methods for Modeling}
There is a suite of numerical methods that have been adopted by the materials science community for computing models of material phenomena. Finite element methods are some of the most common methods. Feedstock, heat source, and melt pool dynamics have been modeled by finite element methods \cite{Toyserkani2004, Khairallah2016, Manvatkar2014} or finite  volume methods \cite{Dai2014}. Some AM-specific tweaks to the finite element method have been developed, such as the quiet/inactive method of Michaeleris et al \cite{Michaleris2014}. They have also been applied to microstructure development \cite{Nie2014}. A review of finite element methods for AM can be found at \cite{Gouge2018}. Models of grain growth in AM have been solved using both phase field numerical methods \cite{Chen2002, Gong2015, Kundin2015, Sahoo2016}, and cellular automata \cite{Tan2011}. Francois et al. provide a review of ICME approaches across spatiotemporal scales\cite{Francois2017}. 

The success of these numerical methods have been in solving complex thermomechanical problems for engineering application. In AM, the number of models that need to be simultaneously considered/computed and the scale of the manufacturing process causes the computational complexity of these methods to grow quickly. Machine learning can make the modeling process more efficient through three primary applications:
\begin{itemize}
	\item Determine a priori which models \textbf{not} to run.
	\item Reduce dimensionality by discarding inputs or physics that are not relevant or that do not have an appreciable impact. 
	\item Compute the same relationship using ML as the numerical method, instead of using explicit methods like FEA, cellular automata, etc.
\end{itemize}

In the case that models \textit{can} be run, but are time-intensive, it behooves the researcher to run as few models as necessary to understand the material response. ML can identify which models will produce the most useful information, informing model choice to the researcher. In another case, ML can be used to identify physics that do not significantly impact the outcome of a model. Reduced-physics models can then be created with a reduced computational burden. In some cases ML can compute the same result as the explicit model with significantly less computational cost (under certain conditions and assumptions).

Dimensionality reduction algorithms identify which parameters are relevant to model in an ICME approach and which are not, enabling future ICME investigations to achieve the same result faster. Materials science has long had a need for dimensionally reduced, computationally accurate models. Some of the first applications of machine learning in materials science was for dimensionality reduction . Dimensionality reduction has been applied to find process-structure-property relations across multiple material length scales \cite{Fischer2006, Flores-Livas2017, Rupp2011, Snyder2012}. Homer applied dimensionality reduction to relate the impact of local atomic environments on mesoscale properties like atomic mobility at grain boundaries, demonstrating the benefit of the technique for advancing ICME \cite{Homer2019}. 

Statistically driven approaches can focus on the parameters in $\mathbf{x}$ that strongly impact AM model outputs, leading to a dynamically guided design of experiments. In design of experiments, a random forest is trained on previously completed experiments. These are the rows of the matrix $\mathbf{B}$ in Eqn. \ref{Bmatrix}. Then, new points in the design space that have not been observed are given to the algorithm and predictions about the output are made. Dimensionality reduction using random forests proceeds differently. 

The random forest is trained on subsets of the data in $\mathbf{B}$. The importance of a feature in the dataset is tested by randomly shuffling the values of one of the columns of $\mathbf{B}$. If randomly shuffling the values of a given feature does not significantly impact the prediction accuracy of the random forest then it is likely that the feature is not important. Towards Data Science provides a more in-depth tutorial on using random forests for feature importance determination \cite{FeatureImportance}. This approach can be applied in computational models that consider many different physics. Several models are run under different initial conditions in the design space. The entires of the matrix $\mathbf{B}$ are the inputs and predicted outcomes of the model. If the exclusion of a model input does not significantly impact the random forest's prediction of the model output, then that input can likely be ignored in future simulations, saving computational time and reducing the number of models that need to be run.

Kamath used a random forest algorithm to screen out irrelevant modeling parameters for predicting maximum density of additively manufactured parts \cite{Kamath2016}. Kamath started with an experimental dataset of manufacturing parameters and multiple modeling methods. An Eagar-Tsai simulation of a Gaussian laser beam on a powder bed was used to model thermal conduction during manufacture, as well as the computationally more expensive Verhaeghe model. The Eagar-Tsai model originally began with four inputs (laser power, speed, beam size, and powder absorptivity) and a design space of 462 possible input combinations. Kamath used random forests to determine which input was most important for achieving fully dense parts. If simulations are time-intensive to run then 462 different simulations may be out of the question. The computational dataset was complemented with an experimental dataset of measured melt pool widths at various printing conditions. Identifying which parameters do not impact the final result reduces the size of input combinations, therefore reducing the number of computations or experiments to be performed. 

Kamath identified that laser speed and power were the most important inputs out of the four to determine melt pool depth and shape. Now that the important physics have been identified, the researchers can proceed to the more expensive Verhaeghe model with knowledge of what parameters to vary.  After determining the most important inputs, the same regression tree was applied to find optimized manufacturing conditions for fully dense parts. However, instead of identifying which features impacted the model standard deviation, the machine settings that maximized $y$ were found. 

A final technique for reducing the burden of computational models requires expressing model data in a matrix and performing matrix factorization. As before, model inputs can be formed into a matrix, $\mathbf{X}$, whose rows are coordinates in the design space. Matrix factorization techniques represent correlations in large datasets in a simplified way. The matrix $\mathbf{X}^T\mathbf{X}$ is a measure of covariance within $\mathbf{X}$. The matrix $\mathbf{X}^T\mathbf{X}$ can be very large due to the design space of additive manufacturing. One type of matrix factorization, called Principal Component Analysis (PCA) represents the data matrix $\mathbf{X}$ as
\eqn
	\mathbf{X} = \mathbf{U} \mathbf{\Sigma}\mathbf{V}^T
	\label{PCA}
\equ
where the columns of $\mathbf{U}$ are the eigenvectors of $\mathbf{X}\mathbf{X}^T$ and are called the \textit{principal components of $\mathbf{X}$}. Similarly, the columns of $\mathbf{V}$ are the eigenvectors of $\mathbf{X}^T\mathbf{X}$. The matrix $\mathbf{\Sigma}$ is a diagonal matrix whose entries are the singular values of $\mathbf{X}$. The first singular value, which corresponds to the first column vector in $\mathbf{V}$, has the highest variance (most information); the second singular value, the second most; and so on. Therefore, regression can be performed on one, a few, or many of the principal components to predict new model results using considerably less data than present in $\mathbf{X}$, but with a minimal loss of information and a minimal reduction in model accuracy.  Materials science studies have used PCA previously to re-represent large datasets in simpler forms, such as predicting the formation energies of crystal structures from a lower dimensional space \cite{Curtarolo2003}. A review of applications of PCA in materials science can be found at \cite{Rajan2009}.

In additive manufacturing, PCA can serve to reduce the number of features in the design space. The new vectors can then be used as regression model inputs for prediction of material properties based on trends observed in dataset $\mathbf{X}$.


